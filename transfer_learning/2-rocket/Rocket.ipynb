{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tsfresh_check = False #@param {type:\"boolean\", run:\"auto\"}\n",
    "\n",
    "import pickle\n",
    "with open('run_tsfresh_check.pickle', 'wb') as f:\n",
    "    pickle.dump(run_tsfresh_check, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages are installed and up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import_errors = []\n",
    "\n",
    "import scipy\n",
    "if scipy.__version__ == '1.4.1':\n",
    "    print(f'Upgrading package: \"{\"scipy\"}\"...')\n",
    "    !pip install scipy>=1.5\n",
    "    import_errors.append('scipy')\n",
    "\n",
    "try:\n",
    "    import tsai\n",
    "except ImportError as e:\n",
    "    print(f'Installing package: \"{\"tsai\"}\"...')\n",
    "    #!pip install -Uqq tsai\n",
    "    !pip install -Uqq git+https://github.com/timeseriesAI/tsai.git\n",
    "    import_errors.append('tsai')\n",
    "\n",
    "if import_errors:\n",
    "    print(f'The following 3rd party packages had to be installed: {import_errors}.')\n",
    "    print('Restarting runtime...')\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print('All packages are installed and up-to-date!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import json\n",
    "\n",
    "from tensorflow.keras import backend as keras\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn import ensemble, preprocessing, svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, AveragePooling2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import LSTM, GRU, RNN\n",
    "from tensorflow.keras.losses import MAE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.compat.v1.keras.backend import set_session as keras_set_session\n",
    "from tensorflow.python.client import device_lib\n",
    "# visible_devices = '1' #this is the GPU number, this is GPU0 ‘0’ or GPU1 ‘1’\n",
    "# memory_fraction = 1.0 #This will allow 20% of the GPU memory to be allocated to your process, pick this number large enough for your script but also not too large so others can still do things.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = visible_devices\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=memory_fraction, allow_growth=True) \n",
    "# tf_session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# keras_set_session(tf_session)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix bug\n",
    "import sys\n",
    "import sklearn\n",
    "#sys.modules['sklearn.ensemble._base'] = sklearn.ensemble.base\n",
    "# Import native packages\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Import auxillary packages\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "# Import machine learning packages\n",
    "import fastai\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, rfft\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sktime.transformations.panel.rocket import MiniRocket as MiniRocket_sktime, MiniRocketMultivariate as MiniRocketMultivariate_sktime\n",
    "\n",
    "import torch\n",
    "\n",
    "#from tsai.all import *\n",
    "#from tsai.all import MiniRocketClassifier as MiniRocket_fastai\n",
    "\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import json\n",
    "\n",
    "from tensorflow.keras import backend as keras\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn import ensemble, preprocessing, svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, AveragePooling2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import LSTM, GRU, RNN\n",
    "from tensorflow.keras.losses import MAE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.compat.v1.keras.backend import set_session as keras_set_session\n",
    "from tensorflow.python.client import device_lib\n",
    "# visible_devices = '1' #this is the GPU number, this is GPU0 ‘0’ or GPU1 ‘1’\n",
    "# memory_fraction = 1.0 #This will allow 20% of the GPU memory to be allocated to your process, pick this number large enough for your script but also not too large so others can still do things.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = visible_devices\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=memory_fraction, allow_growth=True) \n",
    "# tf_session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# keras_set_session(tf_session)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CONNECT TO LOCAL DATABASE\n",
    "#\n",
    "def create_db_connection(db_file_name):\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file_name)\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUEP\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUEP_files/eeg_recordings_TUEP.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           FROM patients \n",
    "           \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUEP = pd.read_sql(query, conn)\n",
    "\n",
    "#TUAB\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUAB_files/eeg_recordings_TUAB.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           ,patients.patient_train_or_test FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUAB = pd.read_sql(query, conn)\n",
    "\n",
    "#TUSZ\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSZ_files/eeg_recordings_TUSZ.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSZ = pd.read_sql(query, conn)\n",
    "\n",
    "#TUSL\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSL_files/eeg_recordings_TUSL.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSL = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"TUEP\": df_TUEP, \"TUAB\": df_TUAB, \"TUSZ\": df_TUSZ, \"TUSL\": df_TUSL}\n",
    "h5f_dict = {\"TUEP\": \"/mnt/disks/data/files/TUEP_files/raw_data_TUEP.h5\", \"TUAB\": \"/mnt/disks/data/files/TUAB_files/raw_data_TUAB.h5\", \"TUSZ\": \"/mnt/disks/data/files/TUSZ_files/raw_data_TUSZ.h5\", \"TUSL\": \"/mnt/disks/data/files/TUSL_files/raw_data_TUSL.h5\"}\n",
    "plus_edf_dict = {\"TUEP\": False, \"TUAB\": True, \"TUSZ\": True, \"TUSL\": True}\n",
    "class1_dict = {\"TUEP\": \"epilepsy\", \"TUAB\": \"abnormal\", \"TUSZ\": \"seiz\", \"TUSL\": \"slow\"}\n",
    "class2_dict = {\"TUEP\":\"no_epilepsy\", \"TUAB\": \"normal\",\"TUSZ\": \"bckg\", \"TUSL\": \"bckg\"}\n",
    "multiple_labels_dict = {\"TUEP\": False, \"TUAB\": False, \"TUSZ\": True, \"TUSL\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sum(l):\n",
    "    window_sum = 0\n",
    "    for windows in l:\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_sum += int(window)\n",
    "    return window_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_list(l):\n",
    "    window_list = []\n",
    "    for windows in l:\n",
    "        window_element = []\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_element.append(int(window))\n",
    "        window_list.append(window_element)\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_windows(l):\n",
    "    total = 0\n",
    "    for window_element in l:\n",
    "        for window in window_element:\n",
    "            total += window\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE THE CUMULATIVE NUMBER OF TOKENS/SECONDS OF RAW DATA PER PATIENT\n",
    "#\n",
    "def get_tokens_cumsum(df, name):\n",
    "    # Get (cumulative) number of tokens per unique epilepsy patient\n",
    "    #       Select patient_ids\n",
    "    #       (Optional TO DO: order patients to assure variation in age and sexe)\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "\n",
    "    #       Iterate over patient_ids and keep track of number of tokens per patient\n",
    "    patient_tokens = []\n",
    "    for curr_patient_id in patient_ids:\n",
    "        curr_patient_tokens = df[df[\"patient_id\"] == curr_patient_id]\n",
    "        if multiple_labels_dict[name]:\n",
    "            curr_patient_windows = window_sum(df[df[\"patient_id\"] == curr_patient_id]['number_of_windows'])\n",
    "            \n",
    "            patient_tokens.append(int(curr_patient_windows))\n",
    "        else:\n",
    "            curr_patient_windows = np.floor(df[df[\"patient_id\"] == curr_patient_id][\"recording_duration\"] / 10)\n",
    "            patient_tokens.append(curr_patient_windows.sum())        \n",
    "        \n",
    "    patient_tokens = np.asarray(patient_tokens)\n",
    "    #       Calculate cumulative number of recordings \n",
    "    patient_tokens_cumsum = np.cumsum(patient_tokens)\n",
    "    \n",
    "    return patient_tokens_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   EXTRACT VALUES AND LABELS FROM TRAIN/VALIDATION/TEST DATASET; TAKES FEATURES TO BE SELECTED AS ARGUMENTS TO FORWARD TO FEATURE SELECTION ALGORITHM\n",
    "#\n",
    "def data_to_values_and_labels(dfs, name):\n",
    "    dfs_values, dfs_labels = [], []\n",
    "        \n",
    "    \n",
    "    # Read in diagnosis and features/feature images from database\n",
    "    for df in dfs:\n",
    "        values = [] # (NR_OF_SAMPLE)\n",
    "        labels = [] # (NR_OF_SAMPLES,)\n",
    "            \n",
    "        rows = df.iterrows()\n",
    "        for row_index, row in rows:\n",
    "            # Return list of filenames to read in with generator\n",
    "            #       Extract file path and name from database\n",
    "            file_name = row[\"file_name\"]\n",
    "            file_path = row[\"file_path\"]\n",
    "\n",
    "            #       Add file path and name to values array\n",
    "            values.append(f\"{'data'}{file_path}{file_name}\")\n",
    "\n",
    "            #       Add patient diagnosis as label to labels array\n",
    "            if multiple_labels_dict[name]:#TODO: make sure this works\n",
    "                label_list = []\n",
    "                diagnoses = row[\"diagnosis\"]\n",
    "                diagnoses = diagnoses.split(\" \")\n",
    "                for diagnosis in diagnoses:\n",
    "                    if diagnosis == class1_dict[name]:\n",
    "                        label_list.append(1)\n",
    "                    elif diagnosis == class2_dict[name]:\n",
    "                        label_list.append(0)\n",
    "                        \n",
    "                labels.append(label_list)\n",
    "            \n",
    "            else:\n",
    "                labels.append(1 if row[\"diagnosis\"] == class1_dict[name] else 0)\n",
    "              \n",
    "        # Add extracted values and labels to arrays for returning \n",
    "        dfs_values.append(values)             \n",
    "        dfs_labels.append(labels)\n",
    "        \n",
    "        \n",
    "    return dfs_values, dfs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    # Filter out empty arrays\n",
    "    df = df_dict[dataset]\n",
    "    if not multiple_labels_dict[dataset]:\n",
    "        df = df[df[\"recording_duration\"] > 10]        \n",
    "    \n",
    "    df_train_all, df_test, df_train, df_val, class1_filters = train_val_test_split2(df, dataset)       \n",
    "\n",
    "    \n",
    "    #SHUFFLE\n",
    "    df_train_all = df_train_all.sample(frac=1).reset_index(drop=True)\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "    df_val = df_val.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Extract data values and labels from individual datasets\n",
    "    dfs = [df_train_all, df_test, df_train, df_val]\n",
    "    dfs_values, dfs_labels = data_to_values_and_labels(dfs, dataset) \n",
    "\n",
    "    #       Extract all values and labels from their respective datasets\n",
    "    train_all_values, train_all_labels = dfs_values[0], dfs_labels[0]\n",
    "    test_values, test_labels = dfs_values[1], dfs_labels[1]\n",
    "    train_values, train_labels = dfs_values[2], dfs_labels[2]\n",
    "    val_values, val_labels = dfs_values[3], dfs_labels[3]\n",
    "    \n",
    "    \n",
    "    train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = np.asarray(train_all_values), np.asarray(train_all_labels), np.asarray(test_values), np.asarray(test_labels), np.asarray(train_values), np.asarray(train_labels), np.asarray(val_values), np.asarray(val_labels)\n",
    "    return [train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class1_filters]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split2(df, dataset):\n",
    "     # Get patient ids\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "    \n",
    "    # Get (cumulative) number of tokens per unique epilepsy patient\n",
    "    patient_tokens_cumsum = get_tokens_cumsum(df, dataset)\n",
    "    total_tokens = np.amax(patient_tokens_cumsum)\n",
    "\n",
    "    \n",
    "    # Split patient_ids based on number of recordings; 80% of all tokens should be training data\n",
    "    # (of which 64% training data and 16% validation data) and 20% should be test data.\n",
    "    #       Find patient_index corresponding to a cumulative 80% of the data to split training from test data\n",
    "    test_split = next(index for index, curr_patient_tokens_cumsum in enumerate(patient_tokens_cumsum) if curr_patient_tokens_cumsum > 0.8 * total_tokens) + 1\n",
    "    \n",
    "    #       Use split patient_id to split off 20% of the data for test data\n",
    "    train_all_patient_ids = patient_ids[:test_split]\n",
    "    train_all_patient_tokens = patient_tokens_cumsum[:test_split]\n",
    "    \n",
    "    test_patient_ids = patient_ids[test_split:]\n",
    "    test_patient_tokens = patient_tokens_cumsum[test_split:]\n",
    "\n",
    "    #       Find patient_id corresponding to a cumulative 80% of the training data to split training data from validation data\n",
    "    total_train_tokens = np.amax(train_all_patient_tokens)\n",
    "    val_split = next(index for index, curr_patient_tokens_cumsum in enumerate(train_all_patient_tokens) if curr_patient_tokens_cumsum > 0.8 * total_train_tokens) + 1\n",
    "\n",
    "    #       Use split patient_id to split off 20% of the data for validation data\n",
    "    train_patient_ids = train_all_patient_ids[:val_split]\n",
    "    train_patient_tokens = train_all_patient_tokens[:val_split]\n",
    "\n",
    "    val_patient_ids = train_all_patient_ids[val_split:]\n",
    "    val_patient_tokens = train_all_patient_tokens[val_split:]\n",
    "\n",
    "    \n",
    "    # Split dataframe in train, validation and test sets based on the split made above\n",
    "    df_train_all = df[df[\"patient_id\"].isin(train_all_patient_ids)]\n",
    "    df_test = df[df[\"patient_id\"].isin(test_patient_ids)]\n",
    "    \n",
    "    df_train = df[df[\"patient_id\"].isin(train_patient_ids)]\n",
    "    df_val = df[df[\"patient_id\"].isin(val_patient_ids)]\n",
    "    \n",
    "    dfs = [df_train_all, df_train, df_val]\n",
    "    class1_filters = []\n",
    "    #stratify this\n",
    "    for df in dfs:\n",
    "        #count how many of each class\n",
    "        number_class0, number_class1 = number_class(df, dataset)            \n",
    "        \n",
    "        if not multiple_labels_dict[dataset]:\n",
    "            #class 0 (majority class) downsamplen\n",
    "            class1_filter = list(range(number_class0))\n",
    "            class1_filter = random.sample(class1_filter, number_class1)\n",
    "            class1_filters.append(class1_filter)\n",
    "        else:\n",
    "            #class 1 (majority class) downsamplen\n",
    "            class1_filter = list(range(number_class1))\n",
    "            class1_filter = random.sample(class1_filter, number_class0)\n",
    "            class1_filters.append(class1_filter)\n",
    "    \n",
    "    return df_train_all, df_test, df_train, df_val, class1_filters      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_class(df, dataset):\n",
    "    number_class0 = 0\n",
    "    number_class1 = 0\n",
    "    \n",
    "    class0 = class1_dict[dataset]\n",
    "    class1 = class2_dict[dataset]\n",
    "    for index, row in df.iterrows():\n",
    "        if not multiple_labels_dict[dataset]:\n",
    "            if class1_dict[dataset] == row[\"diagnosis\"]:\n",
    "                number_class0 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            else:\n",
    "                number_class1 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "        else:\n",
    "            diagnoses = row[\"diagnosis\"]\n",
    "            diagnoses = diagnoses.split(\" \")\n",
    "            diagnoses.remove('')\n",
    "\n",
    "            number_of_windows = row[\"number_of_windows\"]\n",
    "            number_of_windows = number_of_windows.split(\" \")\n",
    "            number_of_windows.remove('')\n",
    "            for i in range(len(diagnoses)):\n",
    "                diagnosis = diagnoses[i]\n",
    "                windows = number_of_windows[i]\n",
    "                if diagnosis == class0:\n",
    "                    number_class0 += int(windows)\n",
    "                elif diagnosis == class1:\n",
    "                    number_class1 += int(windows)\n",
    "    return int(number_class0), int(number_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE RECALL, PRECISION, AND F1-SCORE PERFORMANCE MEASURES\n",
    "#\n",
    "#w = 3.2552225249772935\n",
    "def w_binary_crossentropy(y_true, y_pred):\n",
    "    weights = y_true * class1_weight + (1. - y_true) * class0_weight\n",
    "    bce = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = keras.backend.mean(bce * weights)\n",
    "    return weighted_bce\n",
    "\n",
    "\n",
    "def tp_m(y_true, y_pred):\n",
    "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    return tp\n",
    "\n",
    "def fp_m(y_true, y_pred):\n",
    "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * y_pred, 0, 1)))\n",
    "    return fp\n",
    "\n",
    "def fn_m(y_true, y_pred):\n",
    "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * (1-y_pred), 0, 1)))\n",
    "    return fn\n",
    "\n",
    "def tn_m(y_true, y_pred):\n",
    "    tn = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    return tn\n",
    "\n",
    "def accuracy_m(y_true, y_pred):\n",
    "    accuracy = (tp_m(y_true, y_pred) + tn_m(y_true, y_pred)) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return accuracy\n",
    "\n",
    "def pos_true_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fn_m(y_true, y_pred) \n",
    "\n",
    "def pos_pred_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_true_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_pred_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fn_m(y_true, y_pred)\n",
    "\n",
    "# Sensitivity\n",
    "def recall_m(y_true, y_pred):\n",
    "    recall = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Specificity\n",
    "def neg_recall_m(y_true, y_pred):\n",
    "    neg_recall = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    precision = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Negative predictive value\n",
    "def neg_precision_m(y_true, y_pred):\n",
    "    neg_precision = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+keras.backend.epsilon()))\n",
    "\n",
    "def neg_f1_m(y_true, y_pred):\n",
    "    neg_precision = neg_precision_m(y_true, y_pred)\n",
    "    neg_recall = neg_recall_m(y_true, y_pred)\n",
    "    return 2 * ((neg_precision * neg_recall) / (neg_precision + neg_recall + keras.backend.epsilon()))\n",
    "\n",
    "def balanced_acc_m(y_true, y_pred):\n",
    "    return (recall_m(y_true, y_pred) + neg_recall_m(y_true, y_pred))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_values_and_labels(dataset, file_data_h5, file_path, sample_index, index=None):\n",
    "    file_name = file_path.split(\"/\")[-1][:-4]\n",
    "    if plus_edf_dict[dataset]:\n",
    "        file_name = file_name + '.edf'\n",
    "            \n",
    "    #Get file values\n",
    "    if index is None:\n",
    "        file_values = file_data_h5[file_name][sample_index]   \n",
    "    else:\n",
    "        file_values = file_data_h5[file_name+\"_\"+str(index)][sample_index]   \n",
    "\n",
    "    file_values = file_values.reshape((1, file_values.shape[0], file_values.shape[1]))\n",
    "\n",
    "    return file_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_windows(l):\n",
    "    total = 0\n",
    "    for window_element in l:\n",
    "        for window in window_element:\n",
    "            total += window\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_Data_Generator_Heterogeneous(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, file_paths, file_labels, batch_size, dataset, conv=False, class_filter = None):  \n",
    "        \n",
    "        # Store file paths and corresponding labels\n",
    "        self.file_paths = file_paths\n",
    "        self.file_names = [file_path.split(\"/\")[-1] for file_path in file_paths]\n",
    "        self.file_labels = file_labels\n",
    "        self.dataset = dataset\n",
    "        df = df_dict[dataset]\n",
    "        \n",
    "        #Store class filter and class counters\n",
    "        self.class_filter = class_filter\n",
    "        self.class0_counter = 0\n",
    "        self.class1_counter = 0\n",
    "        \n",
    "        # Store batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #initialize reshape stuff\n",
    "        self.conv = conv\n",
    "        \n",
    "        # Calculate total lentgh of the data\n",
    "        df_gen = df[df[\"file_name\"].isin(self.file_names)]\n",
    "        df_gen = df_gen.set_index(\"file_name\")\n",
    "        df_gen = df_gen.loc[self.file_names]\n",
    "    \n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            self.number_of_windows = window_list(df_gen[\"number_of_windows\"].tolist())\n",
    "            self.total_windows = total_windows(self.number_of_windows)\n",
    "        else:            \n",
    "            self.windows = np.floor((df_gen[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            self.windows_cumsum = self.windows.cumsum()\n",
    "            self.total_windows = self.windows_cumsum.iloc[-1]\n",
    "\n",
    "        # Make heterogeneously randomized list of tuples of file index and sample index in that file\n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_labels = self.file_labels[file_index]\n",
    "                file_paths = []\n",
    "                for i in range(len(file_labels)):\n",
    "                    file_path = self.file_paths[file_index]+'_'+str(i)\n",
    "                    file_paths.append(file_path)\n",
    "                    for sample_index in range(self.number_of_windows[file_index][i]):\n",
    "                        file_and_sample_index.append((file_index, i, sample_index))\n",
    "                        file_and_sample_label.append(file_labels[i])\n",
    "                        \n",
    "                        if file_labels[i] == 0 and self.class0_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class0_counter += 1\n",
    "                        elif file_labels[i] == 1 and self.class1_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class1_counter += 1\n",
    "                        \n",
    "        \n",
    "        else:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_path = self.file_paths[file_index]\n",
    "                file_label = self.file_labels[file_index]\n",
    "                for sample_index in range(self.windows[file_index]):                            \n",
    "                    if file_label == 0 and self.class0_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class0_counter += 1\n",
    "                    elif file_label == 1 and self.class1_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class1_counter += 1\n",
    "                       \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        #Shuffle data\n",
    "        split = list(zip(file_and_sample_index, file_and_sample_label))\n",
    "\n",
    "        random.shuffle(split)\n",
    "\n",
    "        file_and_sample_index, file_and_sample_label = list(zip(*split))\n",
    "\n",
    "        self.split_files_and_samples = []\n",
    "        self.split_labels = []\n",
    "        length = int(np.floor(len(file_and_sample_label)/batch_size))\n",
    "        \n",
    "        for i in range(length):\n",
    "            start_index = batch_size*i\n",
    "            end_index = batch_size*(i+1)\n",
    "            self.split_files_and_samples.append(np.asarray(file_and_sample_index[start_index:end_index]))\n",
    "            self.split_labels.append(np.asarray(file_and_sample_label[start_index:end_index]))\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return (np.ceil((self.class0_counter + self.class1_counter) / float(self.batch_size))-1).astype(np.int)           \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Define function to append values and labels to the return values and labels\n",
    "        def append_values_and_labels(batch_values, file_data_h5, file_path, sample_index, index=None):\n",
    "            file_name = file_path.split(\"/\")[-1][:-4]\n",
    "            if plus_edf_dict[self.dataset]:\n",
    "                file_name = file_name + '.edf'\n",
    "            \n",
    "            #Get file values\n",
    "            if index is None:\n",
    "                file_values = file_data_h5[file_name][sample_index]   \n",
    "            else:\n",
    "                file_values = file_data_h5[file_name+\"_\"+str(index)][sample_index]   \n",
    "            \n",
    "            if self.conv:\n",
    "                file_values = np.swapaxes(file_values, 0, 1)\n",
    "            else:\n",
    "                file_values = file_values.reshape((1, file_values.shape[0], file_values.shape[1]))\n",
    "            batch_values.append(file_values)\n",
    "\n",
    "            return batch_values\n",
    "        \n",
    "        # Read in values for all file paths and duplicate file labels according the the amount of values\n",
    "        #       Select subset of data\n",
    "        batch_files_and_samples = self.split_files_and_samples[idx]\n",
    "        batch_labels = self.split_labels[idx]\n",
    "        \n",
    "        #       Open raw data file\n",
    "        h5f = h5py.File(h5f_dict[self.dataset], 'r')\n",
    "\n",
    "        #       Get values for filepaths in batch\n",
    "        batch_values = []\n",
    "        \n",
    "        if  multiple_labels_dict[self.dataset]:\n",
    "            for file_path_index, index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index, index = index)\n",
    "        else:\n",
    "            for file_path_index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index)\n",
    "            \n",
    "        h5f.close()\n",
    "        self.p = np.random.permutation(len(batch_values))\n",
    "        batch_values, batch_labels = np.asarray(batch_values)[self.p], np.asarray(batch_labels)[self.p]\n",
    "        return batch_values, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train rocket original way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), _RidgeGCV())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-26.9050452709198\n",
      "512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-96970583e224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             )\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sktime/transformations/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, Z)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# Non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# method is possible for a given algorithm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;31m# def inverse_transform(self, Z, X=None):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sktime/transformations/panel/rocket/_minirocket_multivariate.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mset_num_threads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_transform_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mset_num_threads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = \"TUEP\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "    \n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "# Construct pipeline\n",
    "model = make_pipeline(minirocket, classifier)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(start_time - time.time())\n",
    "    \n",
    "#Save weights\n",
    "rocket_model = model.steps[0]\n",
    "parameters = np.array(minirocket.parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    parameters[i] = np.array(parameters[i])\n",
    "print(parameters)\n",
    "np.save(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/speedtest\", parameters)\n",
    "\n",
    "print(\"GET TEST SCORE\")\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save rocket weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-36653b68221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TUSZ\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_all_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_all_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define MINIROCKET transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Multivariate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mminirocket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniRocketMultivariate_sktime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "    \n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "# Construct pipeline\n",
    "model = make_pipeline(minirocket, classifier)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(time.time()-start-time)\n",
    "\n",
    "rocket_model = model.steps[0]\n",
    "parameters = np.array(minirocket.parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    parameters[i] = np.array(parameters[i])\n",
    "print(parameters)\n",
    "np.save(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/speedtest\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUEP\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "    \n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "# Construct pipeline\n",
    "model = make_pipeline(minirocket, classifier)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "rocket_model = model.steps[0]\n",
    "parameters = np.array(minirocket.parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    parameters[i] = np.array(parameters[i])\n",
    "print(parameters)\n",
    "np.save(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUAB\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "    \n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "# Construct pipeline\n",
    "model = make_pipeline(minirocket, classifier)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "rocket_model = model.steps[0]\n",
    "parameters = np.array(minirocket.parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    parameters[i] = np.array(parameters[i])\n",
    "print(parameters)\n",
    "np.save(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_weights\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save RidgeClassifierCV weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"results.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "print(type(model))\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUSZ_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUSZ_weights\", 'rb'))\n",
    "\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUSZ TUSZ: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUEP\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUEP_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUEP_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUSZ TUEP: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUAB\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUAB_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUSZ_TUAB_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUSZ TUAB: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUSZ_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUSZ_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUEP TUSZ: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUEP\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUEP_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUEP_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUEP TUEP: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUAB\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUAB_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUAB_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUEP TUAB: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUSZ_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUSZ_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUAB TUSZ: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUEP\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUEP_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUEP_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUAB TUEP: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TUAB\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "#Train generator\n",
    "batch_size = 512\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset)\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_weights.npy\", allow_pickle=True))\n",
    "    \n",
    "# Define classifier\n",
    "model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "\n",
    "for i in range(len(train_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_train, y_train= train_generator[i]\n",
    "    X_train = X_train.squeeze(axis = 1)\n",
    "    X_train = minirocket.transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUAB_weights\", 'wb'))\n",
    "model = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUAB_weights\", 'rb'))\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = model.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))\n",
    "\n",
    "#save this in a file\n",
    "file1.write(\"TUAB TUAB: \\n\")\n",
    "file1.write(str(acc_test) + \"\\n\")\n",
    "file1.write(str(recall)+ \"\\n\")\n",
    "file1.write(str(neg_recall)+ \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "512\n",
      "1024\n",
      "1536\n",
      "2048\n",
      "2560\n",
      "3072\n",
      "3584\n",
      "4096\n",
      "4608\n",
      "5120\n",
      "5632\n",
      "6144\n",
      "6656\n",
      "7168\n",
      "7680\n",
      "8192\n",
      "8704\n",
      "9216\n",
      "9728\n",
      "10240\n",
      "10752\n",
      "11264\n",
      "11776\n",
      "12288\n",
      "12800\n",
      "13312\n",
      "tf.Tensor(0.6862951, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8945714, shape=(), dtype=float32)\n",
      "tf.Tensor(0.47801876, shape=(), dtype=float32)\n",
      "[[3262 3562]\n",
      " [ 738 6262]]\n"
     ]
    }
   ],
   "source": [
    "#Load in samplesss\n",
    "dataset = \"TUAB\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights.npy\", allow_pickle=True))\n",
    "\n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "classifier = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUAB_weights\", 'rb'))\n",
    "\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = classifier.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "512\n",
      "1024\n",
      "1536\n",
      "2048\n",
      "2560\n",
      "3072\n",
      "3584\n",
      "4096\n",
      "4608\n",
      "5120\n",
      "5632\n",
      "6144\n",
      "6656\n",
      "7168\n",
      "7680\n",
      "8192\n",
      "8704\n",
      "9216\n",
      "9728\n",
      "10240\n",
      "10752\n",
      "tf.Tensor(0.69048333, shape=(), dtype=float32)\n",
      "tf.Tensor(0.46464646, shape=(), dtype=float32)\n",
      "tf.Tensor(0.91632026, shape=(), dtype=float32)\n",
      "[[10140   926]\n",
      " [  106    92]]\n"
     ]
    }
   ],
   "source": [
    "#Load in samplesss\n",
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_weights.npy\", allow_pickle=True))\n",
    "\n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "classifier = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUEP_TUSZ_weights\", 'rb'))\n",
    "\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = classifier.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "512\n",
      "1024\n",
      "1536\n",
      "2048\n",
      "2560\n",
      "3072\n",
      "3584\n",
      "4096\n",
      "4608\n",
      "5120\n",
      "5632\n",
      "6144\n",
      "6656\n",
      "7168\n",
      "7680\n",
      "8192\n",
      "8704\n",
      "9216\n",
      "9728\n",
      "10240\n",
      "10752\n",
      "tf.Tensor(0.6373403, shape=(), dtype=float32)\n",
      "tf.Tensor(0.35736677, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9173138, shape=(), dtype=float32)\n",
      "[[10040   905]\n",
      " [  205   114]]\n"
     ]
    }
   ],
   "source": [
    "#Load in samplesss\n",
    "dataset = \"TUSZ\"\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class_filters = get_data(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "\n",
    "# Define MINIROCKET transformer\n",
    "# Multivariate data\n",
    "minirocket = MiniRocketMultivariate_sktime()\n",
    "X_train, y_train= train_generator[0]\n",
    "X_train = X_train.squeeze(axis = 1)\n",
    "minirocket.fit(X_train, y_train)\n",
    "# Must fit first redundant\n",
    "\n",
    "#set the real parameters\n",
    "minirocket.parameters = tuple(np.load(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_weights.npy\", allow_pickle=True))\n",
    "\n",
    "# Define classifier\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "classifier = pickle.load(open(\"/home/jupyter/time_series_transfer_learning/transfer_learning/2-rocket/model_weights/TUAB_TUSZ_weights\", 'rb'))\n",
    "\n",
    "\n",
    "#Get test score\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)\n",
    "predictions = []\n",
    "gold = []\n",
    "for i in range(len(test_generator)):\n",
    "    print(i*batch_size)\n",
    "    X_test, y_test= test_generator[i]\n",
    "    X_test = X_test.squeeze(axis = 1)\n",
    "    X_test = minirocket.transform(X_test)\n",
    "    r_test = classifier.predict(X_test)\n",
    "\n",
    "    #correct format\n",
    "    for element in r_test:\n",
    "        predictions.append(np.float32(element))\n",
    "    for element in y_test:\n",
    "        gold.append(np.float32(element))\n",
    "\n",
    "predictions = np.asarray(predictions)\n",
    "gold = np.asarray(gold)\n",
    "\n",
    "#print performance measures\n",
    "acc_test = balanced_acc_m(predictions, gold)\n",
    "recall = recall_m(predictions, gold)\n",
    "neg_recall = neg_recall_m(predictions, gold)\n",
    "print(acc_test)\n",
    "print(recall), print(neg_recall)\n",
    "print(confusion_matrix(predictions, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
