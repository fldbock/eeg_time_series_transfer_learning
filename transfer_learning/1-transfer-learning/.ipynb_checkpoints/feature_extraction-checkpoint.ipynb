{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The goal of this document is to set up a pipeline containing data preprocessing (feature selection and onwards), model training, and model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "import json\n",
    "\n",
    "from tensorflow.keras import backend as keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn import ensemble, preprocessing, svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, AveragePooling2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import LSTM, GRU, RNN\n",
    "from tensorflow.keras.losses import MAE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.compat.v1.keras.backend import set_session as keras_set_session\n",
    "from tensorflow.python.client import device_lib\n",
    "# visible_devices = '1' #this is the GPU number, this is GPU0 ‘0’ or GPU1 ‘1’\n",
    "# memory_fraction = 1.0 #This will allow 20% of the GPU memory to be allocated to your process, pick this number large enough for your script but also not too large so others can still do things.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = visible_devices\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=memory_fraction, allow_growth=True) \n",
    "# tf_session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# keras_set_session(tf_session)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6434247174503507507,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9009963991834365351\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 18400631410353735883\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14613293312\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9177454749521534810\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data(base)\n",
    "Load in SQLite database and fetch all tokens (recordings) and the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CONNECT TO LOCAL DATABASE\n",
    "#\n",
    "def create_db_connection(db_file_name):\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file_name)\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TUEP, df_TUAB, df_TUSZ = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUEP\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUEP_files/eeg_recordings_TUEP.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           FROM patients \n",
    "           \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUEP = pd.read_sql(query, conn)\n",
    "\n",
    "#TUAB\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUAB_files/eeg_recordings_TUAB.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           ,patients.patient_train_or_test FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUAB = pd.read_sql(query, conn)\n",
    "\n",
    "#TUSZ\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSZ_files/eeg_recordings_TUSZ.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSZ = pd.read_sql(query, conn)\n",
    "\n",
    "#TUSL\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSL_files/eeg_recordings_TUSL.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSL = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"TUEP\": df_TUEP, \"TUAB\": df_TUAB, \"TUSZ\": df_TUSZ, \"TUSL\": df_TUSL}\n",
    "h5f_dict = {\"TUEP\": \"/mnt/disks/data/files/TUEP_files/raw_data_TUEP.h5\", \"TUAB\": \"/mnt/disks/data/files/TUAB_files/raw_data_TUAB.h5\", \"TUSZ\": \"/mnt/disks/data/files/TUSZ_files/raw_data_TUSZ.h5\", \"TUSL\": \"/mnt/disks/data/files/TUSL_files/raw_data_TUSL.h5\"}\n",
    "plus_edf_dict = {\"TUEP\": False, \"TUAB\": True, \"TUSZ\": True, \"TUSL\": True}\n",
    "class1_dict = {\"TUEP\": \"epilepsy\", \"TUAB\": \"abnormal\", \"TUSZ\": \"seiz\", \"TUSL\": \"slow\"}\n",
    "class2_dict = {\"TUEP\":\"no_epilepsy\", \"TUAB\": \"normal\",\"TUSZ\": \"bckg\", \"TUSL\": \"bckg\"}\n",
    "multiple_labels_dict = {\"TUEP\": False, \"TUAB\": False, \"TUSZ\": True, \"TUSL\": True}\n",
    "\n",
    "model_dict = {\"TUEP\": \"tuep_baseline_a.hdf5\", \"TUAB\": \"tuab_baseline_a_2.hdf5\", \"TUSZ\": \"tusz_baseline_a_3.hdf5\", \"TUSL\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "200\n",
      "1240\n",
      "1119\n"
     ]
    }
   ],
   "source": [
    "#Avoid data leakage\n",
    "dataset1 = \"TUEP\"\n",
    "dataset2 = \"TUAB\"\n",
    "\n",
    "patient_ids1 = np.unique(df_dict[dataset1][\"patient_id\"])\n",
    "patient_ids2 = np.unique(df_dict[dataset2][\"patient_id\"])\n",
    "intersection = list(set(patient_ids1) & set(patient_ids2))\n",
    "print(len(intersection))\n",
    "print(len(patient_ids1))\n",
    "print(len(patient_ids2))\n",
    "#df_TUAB = df_TUAB[df_TUAB[\"patient_id\"].isin(intersection) == False]\n",
    "#print(len(np.unique(df_TUAB[\"patient_id\"])))\n",
    "if len(patient_ids1) > len(patient_ids2):\n",
    "    dataset = dataset1\n",
    "else:\n",
    "    dataset = dataset2\n",
    "\n",
    "df_dict[dataset] = df_dict[dataset][df_dict[dataset][\"patient_id\"].isin(intersection) == False]\n",
    "print(len(np.unique(df_dict[dataset][\"patient_id\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overarching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sum(l):\n",
    "    window_sum = 0\n",
    "    for windows in l:\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_sum += int(window)\n",
    "    return window_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_list(l):\n",
    "    window_list = []\n",
    "    for windows in l:\n",
    "        window_element = []\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_element.append(int(window))\n",
    "        window_list.append(window_element)\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_windows(l):\n",
    "    total = 0\n",
    "    for window_element in l:\n",
    "        for window in window_element:\n",
    "            total += window\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE THE CUMULATIVE NUMBER OF TOKENS/SECONDS OF RAW DATA PER PATIENT\n",
    "#\n",
    "def get_tokens_cumsum(df, name, patient_ids):\n",
    "\n",
    "    #       Iterate over patient_ids and keep track of number of tokens per patient\n",
    "    patient_tokens = []\n",
    "    for curr_patient_id in patient_ids:\n",
    "        curr_patient_tokens = df[df[\"patient_id\"] == curr_patient_id]\n",
    "        if multiple_labels_dict[name]:\n",
    "            curr_patient_windows = window_sum(df[df[\"patient_id\"] == curr_patient_id]['number_of_windows'])\n",
    "            patient_tokens.append(int(curr_patient_windows))\n",
    "        else:\n",
    "            curr_patient_windows = np.floor(df[df[\"patient_id\"] == curr_patient_id][\"recording_duration\"] / 10)\n",
    "            patient_tokens.append(curr_patient_windows.sum())        \n",
    "        \n",
    "    patient_tokens = np.asarray(patient_tokens)\n",
    "\n",
    "    #       Calculate cumulative number of recordings \n",
    "    patient_tokens_cumsum = np.cumsum(patient_tokens)\n",
    "    \n",
    "    return patient_tokens_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   EXTRACT VALUES AND LABELS FROM TRAIN/VALIDATION/TEST DATASET; TAKES FEATURES TO BE SELECTED AS ARGUMENTS TO FORWARD TO FEATURE SELECTION ALGORITHM\n",
    "#\n",
    "def data_to_values_and_labels(dfs, name):\n",
    "    dfs_values, dfs_labels = [], []\n",
    "        \n",
    "    \n",
    "    # Read in diagnosis and features/feature images from database\n",
    "    for df in dfs:\n",
    "        values = [] # (NR_OF_SAMPLE)\n",
    "        labels = [] # (NR_OF_SAMPLES,)\n",
    "            \n",
    "        rows = df.iterrows()\n",
    "        for row_index, row in rows:\n",
    "            # Return list of filenames to read in with generator\n",
    "            #       Extract file path and name from database\n",
    "            file_name = row[\"file_name\"]\n",
    "            file_path = row[\"file_path\"]\n",
    "\n",
    "            #       Add file path and name to values array\n",
    "            values.append(f\"{'data'}{file_path}{file_name}\")\n",
    "\n",
    "            #       Add patient diagnosis as label to labels array\n",
    "            if multiple_labels_dict[name]:#TODO: make sure this works\n",
    "                label_list = []\n",
    "                diagnoses = row[\"diagnosis\"]\n",
    "                diagnoses = diagnoses.split(\" \")\n",
    "                for diagnosis in diagnoses:\n",
    "                    if diagnosis == class1_dict[name]:\n",
    "                        label_list.append(1)\n",
    "                    elif diagnosis == class2_dict[name]:\n",
    "                        label_list.append(0)\n",
    "                        \n",
    "                labels.append(label_list)\n",
    "            \n",
    "            else:\n",
    "                labels.append(1 if row[\"diagnosis\"] == class1_dict[name] else 0)\n",
    "              \n",
    "        # Add extracted values and labels to arrays for returning \n",
    "        dfs_values.append(values)             \n",
    "        dfs_labels.append(labels)\n",
    "        \n",
    "        \n",
    "    return dfs_values, dfs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    # Filter out empty arrays\n",
    "    df = df_dict[dataset]\n",
    "    if not multiple_labels_dict[dataset]:\n",
    "        df = df[df[\"recording_duration\"] > 10]        \n",
    "    \n",
    "    df_train_all, df_test, df_train, df_val = train_val_test_split2(df, dataset)       \n",
    "\n",
    "    \n",
    "    #SHUFFLE\n",
    "    df_train_all = df_train_all.sample(frac=1).reset_index(drop=True)\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "    df_val = df_val.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Extract data values and labels from individual datasets\n",
    "    dfs = [df_train_all, df_test, df_train, df_val]\n",
    "    dfs_values, dfs_labels = data_to_values_and_labels(dfs, dataset) \n",
    "\n",
    "    #       Extract all values and labels from their respective datasets\n",
    "    train_all_values, train_all_labels = dfs_values[0], dfs_labels[0]\n",
    "    test_values, test_labels = dfs_values[1], dfs_labels[1]\n",
    "    train_values, train_labels = dfs_values[2], dfs_labels[2]\n",
    "    val_values, val_labels = dfs_values[3], dfs_labels[3]\n",
    "    \n",
    "    \n",
    "    train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = np.asarray(train_all_values), np.asarray(train_all_labels), np.asarray(test_values), np.asarray(test_labels), np.asarray(train_values), np.asarray(train_labels), np.asarray(val_values), np.asarray(val_labels)\n",
    "    return [train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_Data_Generator_Heterogeneous(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, file_paths, file_labels, batch_size, dataset, conv=False, class_filter = None):  \n",
    "        \n",
    "        # Store file paths and corresponding labels\n",
    "        self.file_paths = file_paths\n",
    "        self.file_names = [file_path.split(\"/\")[-1] for file_path in file_paths]\n",
    "        self.file_labels = file_labels\n",
    "        self.dataset = dataset\n",
    "        df = df_dict[dataset]\n",
    "        \n",
    "        #Store class filter and class counters\n",
    "        self.class_filter = class_filter\n",
    "        self.class0_counter = 0\n",
    "        self.class1_counter = 0\n",
    "        \n",
    "        # Store batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #initialize reshape stuff\n",
    "        self.conv = conv\n",
    "        \n",
    "        # Calculate total lentgh of the data\n",
    "        df_gen = df[df[\"file_name\"].isin(self.file_names)]\n",
    "        df_gen = df_gen.set_index(\"file_name\")\n",
    "        df_gen = df_gen.loc[self.file_names]\n",
    "    \n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            self.number_of_windows = window_list(df_gen[\"number_of_windows\"].tolist())\n",
    "            self.total_windows = total_windows(self.number_of_windows)\n",
    "        else:            \n",
    "            self.windows = np.floor((df_gen[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            self.windows_cumsum = self.windows.cumsum()\n",
    "            self.total_windows = self.windows_cumsum.iloc[-1]\n",
    "\n",
    "        # Make heterogeneously randomized list of tuples of file index and sample index in that file\n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_labels = self.file_labels[file_index]\n",
    "                file_paths = []\n",
    "                for i in range(len(file_labels)):\n",
    "                    file_path = self.file_paths[file_index]+'_'+str(i)\n",
    "                    file_paths.append(file_path)\n",
    "                    for sample_index in range(self.number_of_windows[file_index][i]):\n",
    "                        file_and_sample_index.append((file_index, i, sample_index))\n",
    "                        file_and_sample_label.append(file_labels[i])\n",
    "                        \n",
    "                        if file_labels[i] == 0 and self.class0_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class0_counter += 1\n",
    "                        elif file_labels[i] == 1 and self.class1_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class1_counter += 1\n",
    "                        \n",
    "        \n",
    "        else:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_path = self.file_paths[file_index]\n",
    "                file_label = self.file_labels[file_index]\n",
    "                for sample_index in range(self.windows[file_index]):                            \n",
    "                    if file_label == 0 and self.class0_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class0_counter += 1\n",
    "                    elif file_label == 1 and self.class1_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class1_counter += 1\n",
    "                       \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        #Shuffle data\n",
    "        split = list(zip(file_and_sample_index, file_and_sample_label))\n",
    "\n",
    "        random.shuffle(split)\n",
    "\n",
    "        file_and_sample_index, file_and_sample_label = list(zip(*split))\n",
    "\n",
    "        self.split_files_and_samples = []\n",
    "        self.split_labels = []\n",
    "        length = int(np.floor(len(file_and_sample_label)/batch_size))\n",
    "        \n",
    "        for i in range(length):\n",
    "            start_index = batch_size*i\n",
    "            end_index = batch_size*(i+1)\n",
    "            self.split_files_and_samples.append(np.asarray(file_and_sample_index[start_index:end_index]))\n",
    "            self.split_labels.append(np.asarray(file_and_sample_label[start_index:end_index]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return (np.ceil((self.class0_counter + self.class1_counter) / float(self.batch_size))-1).astype(np.int)           \n",
    "\n",
    "    def get_class_weights(self):\n",
    "        total = self.class0_counter + self.class1_counter\n",
    "        class0_weight = self.class1_counter/total\n",
    "        class1_weight = self.class0_counter/total\n",
    "        return {0 : class0_weight, 1 : class1_weight}\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Define function to append values and labels to the return values and labels\n",
    "        def append_values_and_labels(batch_values, file_data_h5, file_path, sample_index, index=None):\n",
    "            file_name = file_path.split(\"/\")[-1][:-4]\n",
    "            if plus_edf_dict[self.dataset]:\n",
    "                file_name = file_name + '.edf'\n",
    "            \n",
    "            #Get file values\n",
    "            if index is None:\n",
    "                file_values = file_data_h5[file_name][sample_index]   \n",
    "            else:\n",
    "                file_values = file_data_h5[file_name+\"_\"+str(index)][sample_index]   \n",
    "            \n",
    "            if self.conv:\n",
    "                file_values = np.swapaxes(file_values, 0, 1)\n",
    "            else:\n",
    "                file_values = file_values.reshape((1, file_values.shape[0], file_values.shape[1]))\n",
    "            batch_values.append(file_values)\n",
    "\n",
    "            return batch_values\n",
    "        \n",
    "        # Read in values for all file paths and duplicate file labels according the the amount of values\n",
    "        #       Select subset of data\n",
    "        batch_files_and_samples = self.split_files_and_samples[idx]\n",
    "        batch_labels = self.split_labels[idx]\n",
    "        \n",
    "        #       Open raw data file\n",
    "        h5f = h5py.File(h5f_dict[self.dataset], 'r')\n",
    "\n",
    "        #       Get values for filepaths in batch\n",
    "        batch_values = []\n",
    "        \n",
    "        if  multiple_labels_dict[self.dataset]:\n",
    "            for file_path_index, index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index, index = index)\n",
    "        else:\n",
    "            for file_path_index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index)\n",
    "        \n",
    "        h5f.close()\n",
    "        self.p = np.random.permutation(len(batch_values))\n",
    "        batch_values, batch_labels = np.asarray(batch_values)[self.p], np.asarray(batch_labels)[self.p]\n",
    "        return batch_values, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_class(df, dataset):\n",
    "    number_class0 = 0\n",
    "    number_class1 = 0\n",
    "    \n",
    "    class0 = class1_dict[dataset]\n",
    "    class1 = class2_dict[dataset]\n",
    "    for index, row in df.iterrows():\n",
    "        if not multiple_labels_dict[dataset]:\n",
    "            if class1_dict[dataset] == row[\"diagnosis\"]:\n",
    "                number_class0 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            else:\n",
    "                number_class1 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "        else:\n",
    "            diagnoses = row[\"diagnosis\"]\n",
    "            diagnoses = diagnoses.split(\" \")\n",
    "            diagnoses.remove('')\n",
    "\n",
    "            number_of_windows = row[\"number_of_windows\"]\n",
    "            number_of_windows = number_of_windows.split(\" \")\n",
    "            number_of_windows.remove('')\n",
    "            for i in range(len(diagnoses)):\n",
    "                diagnosis = diagnoses[i]\n",
    "                windows = number_of_windows[i]\n",
    "                if diagnosis == class0:\n",
    "                    number_class0 += int(windows)\n",
    "                elif diagnosis == class1:\n",
    "                    number_class1 += int(windows)\n",
    "    return int(number_class0), int(number_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   SPLIT DATA IN ALMOST EQUAL PARTS\n",
    "#\n",
    "def stratify_ensemble_split(df, dataset, split_size):\n",
    "    #shuffle randomly\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Get patient ids\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "\n",
    "    # Get (cumulative) number of tokens per unique patient\n",
    "    patient_tokens_cumsum = get_tokens_cumsum(df, dataset)\n",
    "    \n",
    "    # take approx split size subset of df\n",
    "    split_index = next(index for index, curr_patient_tokens_cumsum in enumerate(patient_tokens_cumsum) if curr_patient_tokens_cumsum > split_size) + 1\n",
    "    stratified_ids = patient_ids[:split_index]\n",
    "    df = df[df[\"patient_id\"].isin(stratified_ids)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   HELP VISUALIZE LEARNING PROGRESSION\n",
    "#\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize = (12,16))\n",
    "    plt.subplot(4,2,1)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    try:\n",
    "        plt.plot(history.epoch, np.array(history.history['acc']),'g-',\n",
    "               label='Train accuracy')\n",
    "        plt.plot(history.epoch, np.array(history.history['val_acc']),'r-',\n",
    "               label = 'Validation accuracy')\n",
    "    except:\n",
    "        plt.plot(history.epoch, np.array(history.history['accuracy_m']),'g-',\n",
    "               label='Train accuracy')\n",
    "        plt.plot(history.epoch, np.array(history.history['val_accuracy_m']),'r-',\n",
    "               label = 'Validation accuracy')\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4,2,2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss minimised by model')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
    "           label='Train loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
    "           label = 'Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,3)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['recall_m']),'g-',\n",
    "           label='Train recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_recall_m']),'r-',\n",
    "           label = 'Validation recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_recall_m']),'g-',\n",
    "           label='Train neg. recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_recall_m']),'r-',\n",
    "           label = 'Validation neg. recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['precision_m']),'g-',\n",
    "           label='Train precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_precision_m']),'r-',\n",
    "           label = 'Validation precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,6)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_precision_m']),'g-',\n",
    "           label='Train neg. precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_precision_m']),'r-',\n",
    "           label = 'Validation neg. precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['f1_m']),'g-',\n",
    "           label='Train F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_f1_m']),'r-',\n",
    "           label = 'Validation F1')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_f1_m']),'g-',\n",
    "           label='Train neg. F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_f1_m']),'r-',\n",
    "           label = 'Validation neg. F1')\n",
    "    plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE RECALL, PRECISION, AND F1-SCORE PERFORMANCE MEASURES\n",
    "#\n",
    "#w = 3.2552225249772935\n",
    "def w_binary_crossentropy(y_true, y_pred):\n",
    "    weights = y_true * class1_weight + (1. - y_true) * class0_weight\n",
    "    bce = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = keras.backend.mean(bce * weights)\n",
    "    return weighted_bce\n",
    "\n",
    "\n",
    "def tp_m(y_true, y_pred):\n",
    "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    return tp\n",
    "\n",
    "def fp_m(y_true, y_pred):\n",
    "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * y_pred, 0, 1)))\n",
    "    return fp\n",
    "\n",
    "def fn_m(y_true, y_pred):\n",
    "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * (1-y_pred), 0, 1)))\n",
    "    return fn\n",
    "\n",
    "def tn_m(y_true, y_pred):\n",
    "    tn = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    return tn\n",
    "\n",
    "def accuracy_m(y_true, y_pred):\n",
    "    accuracy = (tp_m(y_true, y_pred) + tn_m(y_true, y_pred)) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return accuracy\n",
    "\n",
    "def pos_true_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fn_m(y_true, y_pred) \n",
    "\n",
    "def pos_pred_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_true_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_pred_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fn_m(y_true, y_pred)\n",
    "\n",
    "# Sensitivity\n",
    "def recall_m(y_true, y_pred):\n",
    "    recall = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Specificity\n",
    "def neg_recall_m(y_true, y_pred):\n",
    "    neg_recall = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    precision = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Negative predictive value\n",
    "def neg_precision_m(y_true, y_pred):\n",
    "    neg_precision = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+keras.backend.epsilon()))\n",
    "\n",
    "def neg_f1_m(y_true, y_pred):\n",
    "    neg_precision = neg_precision_m(y_true, y_pred)\n",
    "    neg_recall = neg_recall_m(y_true, y_pred)\n",
    "    return 2 * ((neg_precision * neg_recall) / (neg_precision + neg_recall + keras.backend.epsilon()))\n",
    "\n",
    "def balanced_acc_m(y_true, y_pred):\n",
    "    return (recall_m(y_true, y_pred) + neg_recall_m(y_true, y_pred))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    y_actual = np.asarray(y_actual)\n",
    "    y_hat = np.asarray(y_hat)\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(y_hat)): \n",
    "        if len(y_actual.shape) == 2 and y_actual.shape[1] == 2:\n",
    "            y_actual_ = np.argmax(y_actual[i])\n",
    "        else:\n",
    "            y_actual_ = y_actual[i]\n",
    "        \n",
    "        if len(y_hat.shape) == 2 and y_hat.shape[1] == 2:\n",
    "            y_hat_ = np.argmax(y_hat[i])\n",
    "        else:\n",
    "            y_hat_ = y_hat[i]\n",
    "            \n",
    "        if y_actual_==y_hat_==1:\n",
    "            TP += 1\n",
    "        if y_hat_==1 and y_actual_!=y_hat_:\n",
    "            FP += 1\n",
    "        if y_actual_==y_hat_==0:\n",
    "            TN += 1\n",
    "        if y_hat_==0 and y_actual_!=y_hat_:\n",
    "            FN += 1\n",
    "    \n",
    "    print(f\"tp({TP}), fp({FP}), tn({TN}), fn({FN})\")\n",
    "    \n",
    "    accuracy = (TP + TN) / (TN + TP + FP + FN)\n",
    "    recall = TP / (FN + TP)\n",
    "    neg_recall = TN / (FP + TN)\n",
    "    \n",
    "    try:\n",
    "        precision = TP / (FP + TP)\n",
    "    except:\n",
    "        print(\"No positive predictions...\")\n",
    "        precision = 0\n",
    "        \n",
    "    try:\n",
    "        neg_precision = TN / (FN + TN)\n",
    "    except:\n",
    "        print(\"No negative predictions...\")\n",
    "        neg_precision = 0\n",
    "            \n",
    "    try:\n",
    "        f1 = 2 * (recall * precision) / (recall + precision)\n",
    "    except:\n",
    "        f1 = 0\n",
    "        \n",
    "    try:\n",
    "        neg_f1 = 2 * (neg_recall * neg_precision) / (neg_recall + neg_precision)\n",
    "    except:\n",
    "        neg_f1 = 0\n",
    "\n",
    "    aupr = average_precision_score(np.asarray(y_actual), np.asarray(y_hat))\n",
    "    neg_aupr = average_precision_score(np.asarray([1 - y_a for y_a in y_actual]), np.asarray([1 - y_h for y_h in y_hat]))\n",
    "\n",
    "    print(f\"acc={accuracy}, recall={recall}, neg_recall={neg_recall}, precision={precision}, neg_precision={neg_precision}, f1={f1}, neg_f1={neg_f1}, aupr={aupr}, neg_aupr={neg_aupr}\")\n",
    "    \n",
    "    return accuracy, recall, neg_recall, precision, neg_precision, f1, neg_f1, aupr, neg_aupr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(dataset, file_paths, file_labels):\n",
    "    #number of windows of label, number of windows of window \n",
    "    class0 = 0\n",
    "    class1 = 0\n",
    "    \n",
    "    # Store file paths and corresponding labels\n",
    "    file_names = [file_path.split(\"/\")[-1] for file_path in file_paths]\n",
    "    df = df_dict[dataset]\n",
    "        \n",
    "    # Calculate total lentgh of the data\n",
    "    df_gen = df[df[\"file_name\"].isin(file_names)]\n",
    "    df_gen = df_gen.set_index(\"file_name\")\n",
    "    df_gen = df_gen.loc[file_names]\n",
    "    \n",
    "    #calc number of windows\n",
    "    if multiple_labels_dict[dataset]:\n",
    "        number_of_windows = window_list(df_gen[\"number_of_windows\"].tolist())\n",
    "    else:            \n",
    "        windows = np.floor((df_gen[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "        \n",
    "   # Make heterogeneously randomized list of tuples of file index and sample index in that file\n",
    "    if multiple_labels_dict[dataset]:\n",
    "        for file_index in range(len(file_paths)):\n",
    "            this_file_labels = file_labels[file_index]\n",
    "            for i in range(len(this_file_labels)):\n",
    "                if this_file_labels[i] == 0:\n",
    "                    class0 += number_of_windows[file_index][i]\n",
    "                else:\n",
    "                    class1 += number_of_windows[file_index][i]\n",
    "                        \n",
    "        \n",
    "    else:\n",
    "        for file_index in range(len(file_paths)):\n",
    "            file_label = file_labels[file_index]\n",
    "            if file_label == 0:\n",
    "                class0 += windows[file_index]\n",
    "            else:\n",
    "                class1 += windows[file_index]    \n",
    "    \n",
    "    total = class0 + class1\n",
    "    class0_weight = class1/total\n",
    "    class1_weight = class0/total\n",
    "    return {0 : class0_weight, 1 : class1_weight}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(h5f_dict[\"TUEP\"], 'r')\n",
    "#print(h5f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split2(df, dataset):\n",
    "     # Get patient ids\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "    #if dataset == \"TUEP\":\n",
    "     #   random.Random(5).shuffle(patient_ids)\n",
    "        #patient_ids = np.concatenate((patient_ids[:38], patient_ids[111:]))\n",
    "    # Get (cumulative) number of tokens per unique epilepsy patient\n",
    "    patient_tokens_cumsum = get_tokens_cumsum(df, dataset, patient_ids)\n",
    "    total_tokens = np.amax(patient_tokens_cumsum)\n",
    "\n",
    "    # Split patient_ids based on number of recordings; 80% of all tokens should be training data\n",
    "    # (of which 64% training data and 16% validation data) and 20% should be test data.\n",
    "    #       Find patient_index corresponding to a cumulative 80% of the data to split training from test data      \n",
    "    test_split = next(index for index, curr_patient_tokens_cumsum in enumerate(patient_tokens_cumsum) if curr_patient_tokens_cumsum > 0.8 * total_tokens) + 1\n",
    "        \n",
    "    \n",
    "    #       Use split patient_id to split off 20% of the data for test data\n",
    "    train_all_patient_ids = patient_ids[:test_split]\n",
    "    train_all_patient_tokens = patient_tokens_cumsum[:test_split]\n",
    "    \n",
    "    test_patient_ids = patient_ids[test_split:]\n",
    "    test_patient_tokens = patient_tokens_cumsum[test_split:]\n",
    "\n",
    "    #       Find patient_id corresponding to a cumulative 80% of the training data to split training data from validation data\n",
    "    total_train_tokens = np.amax(train_all_patient_tokens)\n",
    "    random.Random(2).shuffle(train_all_patient_ids)\n",
    "    train_all_patient_tokens = get_tokens_cumsum(df, dataset,train_all_patient_ids)\n",
    "    val_split = next(index for index, curr_patient_tokens_cumsum in enumerate(train_all_patient_tokens) if curr_patient_tokens_cumsum > 0.8 * total_train_tokens) + 1\n",
    "\n",
    "    #       Use split patient_id to split off 20% of the data for validation data\n",
    "    train_patient_ids = train_all_patient_ids[:val_split]\n",
    "    train_patient_tokens = train_all_patient_tokens[:val_split]\n",
    "\n",
    "    val_patient_ids = train_all_patient_ids[val_split:]\n",
    "    val_patient_tokens = train_all_patient_tokens[val_split:]\n",
    "    print(val_split), print(test_split), print(len(patient_ids))\n",
    "    \n",
    "    # Split dataframe in train, validation and test sets based on the split made above\n",
    "    df_train_all = df[df[\"patient_id\"].isin(train_all_patient_ids)]\n",
    "    df_test = df[df[\"patient_id\"].isin(test_patient_ids)]\n",
    "    \n",
    "    df_train = df[df[\"patient_id\"].isin(train_patient_ids)]\n",
    "    df_val = df[df[\"patient_id\"].isin(val_patient_ids)]\n",
    "    \n",
    "    return df_train_all, df_test, df_train, df_val   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, neg_recall_m, precision_m, neg_precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUEP\"\n",
    "target_model = \"TUAB\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-15 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_3 (Dropout)          (None, 1, 19, 2560)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 19, 4)          1310724   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1, 19, 4)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 19, 4)          4         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 2, 1, 4)           40        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 1, 4)           8         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 2, 1, 8)           24        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 1, 8)           8         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,310,811\n",
      "Trainable params: 3\n",
      "Non-trainable params: 1,310,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n",
      "907\n",
      "1119\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.3468 - balanced_acc_m: 0.5015 - pos_pred_m: 109.8312 - neg_pred_m: 146.1688 - tp_m: 55.0519 - fp_m: 54.7792 - tn_m: 73.1299 - fn_m: 73.0390 - recall_m: 0.4306 - neg_recall_m: 0.5724 - precision_m: 0.5037 - neg_precision_m: 0.4983"
     ]
    }
   ],
   "source": [
    "epochs = 65\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuep_tuab_1blockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuep_tuab_1blockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n",
      "907\n",
      "1119\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUAB\"\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuab_tuep_noblockfrozen.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 13s 223ms/step - loss: 0.9465 - balanced_acc_m: 0.5306 - pos_pred_m: 46.5357 - neg_pred_m: 209.4643 - tp_m: 35.7857 - fp_m: 10.7500 - tn_m: 66.6964 - fn_m: 142.7679 - recall_m: 0.2003 - neg_recall_m: 0.8610 - precision_m: 0.7691 - neg_precision_m: 0.3183\n",
      "balanced accuracy 0.5306335687637329\n",
      "aupr 0.693327211733366\n",
      "recall 0.20028363168239594 precision 0.7690843343734741 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"balanced accuracy\", test_dict['balanced_acc_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUEP\"\n",
    "target_model = \"TUAB\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-4 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuep_tuab_3blockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuep_tuab_3blockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUEP\"\n",
    "target_model = \"TUAB\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-10 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuep_tuab_2blockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuep_tuab_2blockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUEP\"\n",
    "target_model = \"TUAB\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuep_tuab_noblockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuep_tuab_noblockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUAB\"\n",
    "target_model = \"TUEP\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-15 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuab_tuep_1blockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuab_tuep_1blockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUAB\"\n",
    "target_model = \"TUEP\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-10 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuab_tuep_2blockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuab_tuep_2blockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUAB\"\n",
    "target_model = \"TUEP\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-4 and not layer.__class__.__name__ == \"Dropout\":\n",
    "        layer.set_weights(clone.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "    i += 1\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuab_tuep_3blockfrozen_val.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuab_tuep_3blockfrozen_val.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = \"/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/\"\n",
    "source_model = \"TUAB\"\n",
    "target_model = \"TUEP\"\n",
    "clone = get_eegnet()\n",
    "clone.load_weights(basepath + model_dict[source_model])\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "dataset = target_model\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = train_generator.get_class_weights()\n",
    "\n",
    "\n",
    "epochs = 90\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/model_weights/tuab_tuep_noblockfrozen.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_tuab_tuep_noblockfrozen.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   HELP VISUALIZE LEARNING PROGRESSION\n",
    "#\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize = (12,16))\n",
    "    plt.subplot(2,2,1)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    plt.plot(history.epoch, np.array(history.history['balanced_acc_m']),'g-', label='Train accuracy')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_balanced_acc_m']),'r-', label = 'Validation accuracy')\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss minimised by model')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
    "           label='Train loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
    "           label = 'Validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 88ms/step - loss: 0.7348 - accuracy_m: 0.3411 - recall_m: 0.3282 - neg_recall_m: 0.6694 - precision_m: 0.9626 - neg_precision_m: 0.0369 - f1_m: 0.4879 - neg_f1_m: 0.0696 - pos_pred_m: 42.0000 - neg_pred_m: 86.0000\n",
      "accuracy 0.3411458432674408\n",
      "aupr 0.9678087810374134\n",
      "recall 0.32824501395225525 precision 0.9625529646873474 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"accuracy\", test_dict['accuracy_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1719666419223027\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUSZ\"\n",
    "df = df_dict[dataset]\n",
    "\n",
    "windows_class1 = 0\n",
    "windows_class2 = 0\n",
    "\n",
    "# Gather all epileptic and non-epileptic patients in separate dataframes\n",
    "if dataset == \"TUEP\":\n",
    "    df_class1 = df[df[\"diagnosis\"]==\"epilepsy\"]\n",
    "    df_class2 = df[df[\"diagnosis\"]==\"no_epilepsy\"]\n",
    "elif dataset == \"TUAB\":\n",
    "    df_class1 = df[df[\"diagnosis\"]==\"abnormal\"]\n",
    "    df_class2 = df[df[\"diagnosis\"]==\"normal\"]\n",
    "\n",
    "# Calculate windows and how many of each class there are\n",
    "if dataset == \"TUEP\" or dataset == \"TUAB\":\n",
    "    class1_cumsum = get_tokens_cumsum(df_class1, dataset)\n",
    "    class2_cumsum = get_tokens_cumsum(df_class2, dataset)\n",
    "    print(np.amax(class1_cumsum), np.amax(class2_cumsum))\n",
    "\n",
    "else:#TUSZ \n",
    "    print(window_sum_class(df))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>number_of_windows</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>electrode_setup</th>\n",
       "      <th>recording_duration</th>\n",
       "      <th>sampling_freq</th>\n",
       "      <th>len_of_samples</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s002</td>\n",
       "      <td>00000258_s002_t000</td>\n",
       "      <td>5</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>20.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s002_t000.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s002_2003_07_21/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s002</td>\n",
       "      <td>00000258_s002_t002</td>\n",
       "      <td>105</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>272.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s002_t002.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s002_2003_07_21/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t000</td>\n",
       "      <td>90</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>234.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t000.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t001</td>\n",
       "      <td>86</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>224.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t001.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t002</td>\n",
       "      <td>0 62 8</td>\n",
       "      <td>bckg seiz bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>203.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t002.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id session_id            token_id number_of_windows  \\\n",
       "0   00000258       s002  00000258_s002_t000                 5   \n",
       "1   00000258       s002  00000258_s002_t002               105   \n",
       "2   00000258       s003  00000258_s003_t000                90   \n",
       "3   00000258       s003  00000258_s003_t001                86   \n",
       "4   00000258       s003  00000258_s003_t002            0 62 8   \n",
       "\n",
       "         diagnosis electrode_setup  recording_duration  sampling_freq  \\\n",
       "0            bckg        01_tcp_ar                20.0            400   \n",
       "1            bckg        01_tcp_ar               272.0            400   \n",
       "2            bckg        01_tcp_ar               234.0            400   \n",
       "3            bckg        01_tcp_ar               224.0            400   \n",
       "4  bckg seiz bckg        01_tcp_ar               203.0            400   \n",
       "\n",
       "   len_of_samples               file_name  \\\n",
       "0               1  00000258_s002_t000.edf   \n",
       "1               1  00000258_s002_t002.edf   \n",
       "2               1  00000258_s003_t000.edf   \n",
       "3               1  00000258_s003_t001.edf   \n",
       "4               1  00000258_s003_t002.edf   \n",
       "\n",
       "                                      file_path  \n",
       "0  /dev/01_tcp_ar/002/00000258/s002_2003_07_21/  \n",
       "1  /dev/01_tcp_ar/002/00000258/s002_2003_07_21/  \n",
       "2  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  \n",
       "3  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  \n",
       "4  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"TUSZ\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sum_class(l):\n",
    "    #number_of_windows = l['number_of_windows']\n",
    "    windows_class1 = 0\n",
    "    windows_class2 = 0\n",
    "    \n",
    "    for index, data in l.iterrows():\n",
    "        windows = data[\"number_of_windows\"]\n",
    "        windows = windows.split(\" \")\n",
    "        \n",
    "        diagnoses = data[\"diagnosis\"]\n",
    "        diagnoses = diagnoses.split(\" \")\n",
    "    \n",
    "        for i in range(len(windows)):\n",
    "            if not windows[i] == '':\n",
    "                if diagnoses[i] == \"seiz\":\n",
    "                    windows_class1 += int(windows[i])\n",
    "                else:\n",
    "                    windows_class2 += int(windows[i])\n",
    "    return windows_class1, windows_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'number_of_windows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'number_of_windows'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b5d81f5707d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_sum_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_TUEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-c2db8aeadfa9>\u001b[0m in \u001b[0;36mwindow_sum_class\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"number_of_windows\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'number_of_windows'"
     ]
    }
   ],
   "source": [
    "print(window_sum_class(df_TUEP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-17447c92b48b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class1_filters = get_data(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mh5f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5f_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#+ \".edf_0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_values' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "dataset = \"TUSZ\"\n",
    "i = 0\n",
    "#train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class1_filters = get_data(dataset)\n",
    "h5f = h5py.File(h5f_dict[dataset], 'r')\n",
    "file_name = train_values[i].split(\"/\")[-1]\n",
    "file_name = file_name.split(\"/\")[-1][:-4] #+ \".edf_0\"\n",
    "print(file_name)\n",
    "print(train_labels[i])\n",
    "print(h5f[file_name].shape)\n",
    "plt.plot(h5f[file_name][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dense():\n",
    "    model = Sequential()\n",
    "    \n",
    "    do = [0, 0]\n",
    "    \n",
    "    #Block 1\n",
    "    model.add(Flatten(input_shape=(1, 19, 2560)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(do[1]))\n",
    "    \n",
    "    #Block 2\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(do[1]))\n",
    "    \n",
    "    #Block 3\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "def get_plot(plot_path, begin = None, end = None):\n",
    "    training_loss = []\n",
    "    training_acc = []\n",
    "\n",
    "    val_acc = []\n",
    "\n",
    "    with open(plot_path,'r', encoding=\"utf8\", errors='ignore') as csvfile:\n",
    "        lines = csv.reader(csvfile, delimiter=',')\n",
    "        next(lines)\n",
    "        for row in lines:\n",
    "            training_loss.append(float(row[4]))\n",
    "            training_acc.append(float(row[1]))\n",
    "            val_acc.append(float(row[11]))#11 or 14\n",
    "    if end:\n",
    "        training_loss = training_loss[begin+1: end]\n",
    "        training_acc = training_acc[begin + 1: end]\n",
    "        val_acc = val_acc[begin + 1: end]\n",
    "    epochs = range(len(training_loss))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    \n",
    "    ax1.plot(epochs, training_loss, color = 'g')\n",
    "    ax1.title.set_text(\"Loss\")\n",
    "\n",
    "    ax2.plot(epochs, training_acc, color = 'g')\n",
    "    ax2.plot(epochs, val_acc, color = 'r')\n",
    "    ax2.title.set_text(\"Balanced Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/logbooks/logbook_tuep_val_400epochs.csv'\n",
    "#plot_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/9-multi-source-transfer-learning/logbooks/logbook_tuep_tusz_tuab_val.csv'\n",
    "get_plot(plot_path)#, 6, 205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with easy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "input = make_blobs(10*256, 19*2560, centers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = input[0]\n",
    "input_values = np.asarray(list(input_values.reshape(2560, 1, 19, 2560)))\n",
    "\n",
    "input_labels = np.asarray(input[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "#<class 'list'>\n",
    "#<class 'numpy.ndarray'>\n",
    "#<class 'numpy.ndarray'>\n",
    "#<class 'numpy.ndarray'>\n",
    "#<class 'numpy.float64'>\n",
    "print(type(input_values))\n",
    "print(type(input_values[0]))\n",
    "print(type(input_values[0][0]))\n",
    "print(type(input_values[0][0][0]))\n",
    "print(type(input_values[0][0][0][0]))\n",
    "\n",
    "print(type(input_labels))\n",
    "print(type(input_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, neg_recall_m, precision_m, neg_precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "my_model = get_eegnet()\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, test_values = input_values[:2048], input_values[2048:]\n",
    "train_all_labels, test_labels = input_labels[:2048], input_labels[2048:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/64 [===========================>..] - ETA: 0s - loss: 0.1090 - balanced_acc_m: 0.9741 - pos_pred_m: 15.1833 - neg_pred_m: 16.8167 - tp_m: 15.1000 - fp_m: 0.0833 - tn_m: 16.0833 - fn_m: 0.7333 - recall_m: 0.9535 - neg_recall_m: 0.9947 - precision_m: 0.9950 - neg_precision_m: 0.9578\n",
      "Epoch 00001: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.1086 - balanced_acc_m: 0.9744 - pos_pred_m: 15.2969 - neg_pred_m: 16.7031 - tp_m: 15.2188 - fp_m: 0.0781 - tn_m: 15.9688 - fn_m: 0.7344 - recall_m: 0.9538 - neg_recall_m: 0.9950 - precision_m: 0.9953 - neg_precision_m: 0.9572\n",
      "Epoch 2/5\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0982 - balanced_acc_m: 0.9767 - pos_pred_m: 15.4286 - neg_pred_m: 16.5714 - tp_m: 15.3175 - fp_m: 0.1111 - tn_m: 15.9206 - fn_m: 0.6508 - recall_m: 0.9594 - neg_recall_m: 0.9941 - precision_m: 0.9920 - neg_precision_m: 0.9606\n",
      "Epoch 00002: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.0972 - balanced_acc_m: 0.9771 - pos_pred_m: 15.4219 - neg_pred_m: 16.5781 - tp_m: 15.3125 - fp_m: 0.1094 - tn_m: 15.9375 - fn_m: 0.6406 - recall_m: 0.9600 - neg_recall_m: 0.9942 - precision_m: 0.9922 - neg_precision_m: 0.9612\n",
      "Epoch 3/5\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.0983 - balanced_acc_m: 0.9748 - pos_pred_m: 15.4839 - neg_pred_m: 16.5161 - tp_m: 15.3387 - fp_m: 0.1452 - tn_m: 15.8387 - fn_m: 0.6774 - recall_m: 0.9579 - neg_recall_m: 0.9917 - precision_m: 0.9906 - neg_precision_m: 0.9594\n",
      "Epoch 00003: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 0.0991 - balanced_acc_m: 0.9740 - pos_pred_m: 15.3906 - neg_pred_m: 16.6094 - tp_m: 15.2500 - fp_m: 0.1406 - tn_m: 15.9062 - fn_m: 0.7031 - recall_m: 0.9560 - neg_recall_m: 0.9920 - precision_m: 0.9909 - neg_precision_m: 0.9583\n",
      "Epoch 4/5\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.0980 - balanced_acc_m: 0.9662 - pos_pred_m: 15.3968 - neg_pred_m: 16.6032 - tp_m: 15.0794 - fp_m: 0.3175 - tn_m: 15.7937 - fn_m: 0.8095 - recall_m: 0.9504 - neg_recall_m: 0.9820 - precision_m: 0.9791 - neg_precision_m: 0.9524\n",
      "Epoch 00004: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 0.0972 - balanced_acc_m: 0.9667 - pos_pred_m: 15.4688 - neg_pred_m: 16.5312 - tp_m: 15.1562 - fp_m: 0.3125 - tn_m: 15.7344 - fn_m: 0.7969 - recall_m: 0.9511 - neg_recall_m: 0.9823 - precision_m: 0.9794 - neg_precision_m: 0.9531\n",
      "Epoch 5/5\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.0757 - balanced_acc_m: 0.9781 - pos_pred_m: 15.4426 - neg_pred_m: 16.5574 - tp_m: 15.2787 - fp_m: 0.1639 - tn_m: 16.0000 - fn_m: 0.5574 - recall_m: 0.9652 - neg_recall_m: 0.9909 - precision_m: 0.9894 - neg_precision_m: 0.9668\n",
      "Epoch 00005: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5\n",
      "64/64 [==============================] - 1s 10ms/step - loss: 0.0767 - balanced_acc_m: 0.9769 - pos_pred_m: 15.5000 - neg_pred_m: 16.5000 - tp_m: 15.3438 - fp_m: 0.1562 - tn_m: 15.8906 - fn_m: 0.6094 - recall_m: 0.9625 - neg_recall_m: 0.9913 - precision_m: 0.9899 - neg_precision_m: 0.9633\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_test_data1.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit(train_all_values, train_all_labels, shuffle=True, callbacks = active_callbacks, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "input = make_blobs(10*256, 19*2560, centers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = input[0]\n",
    "input_values = np.asarray(list(input_values.reshape(2560, 1, 19, 2560)))\n",
    "\n",
    "input_labels = np.asarray(input[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, neg_recall_m, precision_m, neg_precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source model\n",
    "basepath = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data1.hdf5'\n",
    "my_model = get_eegnet()\n",
    "my_model.load_weights(basepath)\n",
    "\n",
    "#freeze layers\n",
    "nr_layers = len(my_model.layers)\n",
    "i = 0\n",
    "for layer in my_model.layers:\n",
    "    layer.trainable = True\n",
    "    if i < nr_layers-15:\n",
    "        layer.trainable = False\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, test_values = input_values[:2048], input_values[2048:]\n",
    "train_all_labels, test_labels = input_labels[:2048], input_labels[2048:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.1217 - balanced_acc_m: 0.9632 - pos_pred_m: 15.7049 - neg_pred_m: 16.2951 - tp_m: 15.2787 - fp_m: 0.4262 - tn_m: 15.5082 - fn_m: 0.7869 - recall_m: 0.9568 - neg_recall_m: 0.9696 - precision_m: 0.9730 - neg_precision_m: 0.9501\n",
      "Epoch 00001: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.1178 - balanced_acc_m: 0.9649 - pos_pred_m: 15.7656 - neg_pred_m: 16.2344 - tp_m: 15.3594 - fp_m: 0.4062 - tn_m: 15.4844 - fn_m: 0.7500 - recall_m: 0.9588 - neg_recall_m: 0.9711 - precision_m: 0.9743 - neg_precision_m: 0.9524\n",
      "Epoch 2/5\n",
      "59/64 [==========================>...] - ETA: 0s - loss: 0.0647 - balanced_acc_m: 0.9792 - pos_pred_m: 16.2542 - neg_pred_m: 15.7458 - tp_m: 15.8644 - fp_m: 0.3898 - tn_m: 15.4068 - fn_m: 0.3390 - recall_m: 0.9804 - neg_recall_m: 0.9781 - precision_m: 0.9750 - neg_precision_m: 0.9790\n",
      "Epoch 00002: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 0.0645 - balanced_acc_m: 0.9795 - pos_pred_m: 16.1719 - neg_pred_m: 15.8281 - tp_m: 15.7812 - fp_m: 0.3906 - tn_m: 15.5000 - fn_m: 0.3281 - recall_m: 0.9808 - neg_recall_m: 0.9783 - precision_m: 0.9746 - neg_precision_m: 0.9798\n",
      "Epoch 3/5\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.0657 - balanced_acc_m: 0.9801 - pos_pred_m: 15.9016 - neg_pred_m: 16.0984 - tp_m: 15.6066 - fp_m: 0.2951 - tn_m: 15.6721 - fn_m: 0.4262 - recall_m: 0.9763 - neg_recall_m: 0.9838 - precision_m: 0.9794 - neg_precision_m: 0.9716\n",
      "Epoch 00003: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.0665 - balanced_acc_m: 0.9796 - pos_pred_m: 15.9688 - neg_pred_m: 16.0312 - tp_m: 15.6719 - fp_m: 0.2969 - tn_m: 15.5938 - fn_m: 0.4375 - recall_m: 0.9756 - neg_recall_m: 0.9836 - precision_m: 0.9794 - neg_precision_m: 0.9706\n",
      "Epoch 4/5\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.0670 - balanced_acc_m: 0.9792 - pos_pred_m: 16.0645 - neg_pred_m: 15.9355 - tp_m: 15.6935 - fp_m: 0.3710 - tn_m: 15.5806 - fn_m: 0.3548 - recall_m: 0.9798 - neg_recall_m: 0.9787 - precision_m: 0.9766 - neg_precision_m: 0.9774\n",
      "Epoch 00004: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 0.0664 - balanced_acc_m: 0.9799 - pos_pred_m: 16.1250 - neg_pred_m: 15.8750 - tp_m: 15.7656 - fp_m: 0.3594 - tn_m: 15.5312 - fn_m: 0.3438 - recall_m: 0.9804 - neg_recall_m: 0.9794 - precision_m: 0.9773 - neg_precision_m: 0.9781\n",
      "Epoch 5/5\n",
      "60/64 [===========================>..] - ETA: 0s - loss: 0.0616 - balanced_acc_m: 0.9818 - pos_pred_m: 16.0333 - neg_pred_m: 15.9667 - tp_m: 15.7333 - fp_m: 0.3000 - tn_m: 15.6500 - fn_m: 0.3167 - recall_m: 0.9808 - neg_recall_m: 0.9828 - precision_m: 0.9802 - neg_precision_m: 0.9801\n",
      "Epoch 00005: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5\n",
      "64/64 [==============================] - 1s 10ms/step - loss: 0.0601 - balanced_acc_m: 0.9830 - pos_pred_m: 16.0938 - neg_pred_m: 15.9062 - tp_m: 15.8125 - fp_m: 0.2812 - tn_m: 15.6094 - fn_m: 0.2969 - recall_m: 0.9820 - neg_recall_m: 0.9839 - precision_m: 0.9815 - neg_precision_m: 0.9814\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/test_data2_transfer_learning.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/1-transfer-learning/logbooks/logbook_test_data2_transfer_learning', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit(train_all_values, train_all_labels, shuffle=True, callbacks = active_callbacks, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "file_extension": ".py",
  "interpreter": {
   "hash": "1ccaac5b7a573f1cdba5f7b708c2c32d325a2cd4ec145a76750e10d63a8e887c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
