{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The goal of this document is to set up a pipeline containing data preprocessing (feature selection and onwards), model training, and model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "import json\n",
    "\n",
    "from tensorflow.keras import backend as keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn import ensemble, preprocessing, svm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, AveragePooling2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import LSTM, GRU, RNN\n",
    "from tensorflow.keras.losses import MAE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.compat.v1.keras.backend import set_session as keras_set_session\n",
    "from tensorflow.python.client import device_lib\n",
    "# visible_devices = '1' #this is the GPU number, this is GPU0 ‘0’ or GPU1 ‘1’\n",
    "# memory_fraction = 1.0 #This will allow 20% of the GPU memory to be allocated to your process, pick this number large enough for your script but also not too large so others can still do things.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = visible_devices\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=memory_fraction, allow_growth=True) \n",
    "# tf_session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# keras_set_session(tf_session)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14223190267447332494,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 16605368594496606364\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11166382575934160644\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14613293312\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 2515039136054227471\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data(base)\n",
    "Load in SQLite database and fetch all tokens (recordings) and the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CONNECT TO LOCAL DATABASE\n",
    "#\n",
    "def create_db_connection(db_file_name):\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file_name)\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TUEP, df_TUAB, df_TUSZ = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUEP\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUEP_files/eeg_recordings_TUEP.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           FROM patients \n",
    "           \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUEP = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUAB\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUAB_files/eeg_recordings_TUAB.db')\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, patients.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "           ,patients.patient_train_or_test FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUAB = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUSZ\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSZ_files/eeg_recordings_TUSZ.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSZ = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUSL\n",
    "conn = create_db_connection('/mnt/disks/data/files/TUSL_files/eeg_recordings_TUSL.db')\n",
    "\n",
    "query = \"\"\"SELECT patients.patient_id, sessions.session_id, tokens.token_id, tokens.number_of_windows, tokens.diagnosis, sessions.electrode_setup, tokens.recording_duration, tokens.sampling_freq, tokens.len_of_samples, tokens.file_name, tokens.file_path\n",
    "            FROM patients \n",
    "           INNER JOIN sessions ON patients.patient_id == sessions.patient_id \n",
    "           INNER JOIN tokens ON sessions.patient_id == tokens.patient_id AND sessions.session_id == tokens.session_id\"\"\"\n",
    "cur = conn.cursor()\n",
    "df_TUSL = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"TUEP\": df_TUEP, \"TUAB\": df_TUAB, \"TUSZ\": df_TUSZ, \"TUSL\": df_TUSL}\n",
    "h5f_dict = {\"TUEP\": \"/mnt/disks/data/files/TUEP_files/raw_data_TUEP.h5\", \"TUAB\": \"/mnt/disks/data/files/TUAB_files/raw_data_TUAB.h5\", \"TUSZ\": \"/mnt/disks/data/files/TUSZ_files/raw_data_TUSZ.h5\", \"TUSL\": \"/mnt/disks/data/files/TUSL_files/raw_data_TUSL.h5\"}\n",
    "plus_edf_dict = {\"TUEP\": False, \"TUAB\": True, \"TUSZ\": True, \"TUSL\": True}\n",
    "class1_dict = {\"TUEP\": \"epilepsy\", \"TUAB\": \"abnormal\", \"TUSZ\": \"seiz\", \"TUSL\": \"slow\"}\n",
    "class2_dict = {\"TUEP\":\"no_epilepsy\", \"TUAB\": \"normal\",\"TUSZ\": \"bckg\", \"TUSL\": \"bckg\"}\n",
    "multiple_labels_dict = {\"TUEP\": False, \"TUAB\": False, \"TUSZ\": True, \"TUSL\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overarching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sum(l):\n",
    "    window_sum = 0\n",
    "    for windows in l:\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_sum += int(window)\n",
    "    return window_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_list(l):\n",
    "    window_list = []\n",
    "    for windows in l:\n",
    "        window_element = []\n",
    "        windows = windows.split(\" \")\n",
    "        for window in windows:\n",
    "            if not window == '':\n",
    "                window_element.append(int(window))\n",
    "        window_list.append(window_element)\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_windows(l):\n",
    "    total = 0\n",
    "    for window_element in l:\n",
    "        for window in window_element:\n",
    "            total += window\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE THE CUMULATIVE NUMBER OF TOKENS/SECONDS OF RAW DATA PER PATIENT\n",
    "#\n",
    "def get_tokens_cumsum(df, name):\n",
    "    # Get (cumulative) number of tokens per unique epilepsy patient\n",
    "    #       Select patient_ids\n",
    "    #       (Optional TO DO: order patients to assure variation in age and sexe)\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "\n",
    "    #       Iterate over patient_ids and keep track of number of tokens per patient\n",
    "    patient_tokens = []\n",
    "    for curr_patient_id in patient_ids:\n",
    "        curr_patient_tokens = df[df[\"patient_id\"] == curr_patient_id]\n",
    "        if multiple_labels_dict[name]:\n",
    "            curr_patient_windows = window_sum(df[df[\"patient_id\"] == curr_patient_id]['number_of_windows'])\n",
    "            patient_tokens.append(int(curr_patient_windows))\n",
    "        else:\n",
    "            curr_patient_windows = np.floor(df[df[\"patient_id\"] == curr_patient_id][\"recording_duration\"] / 10)\n",
    "            patient_tokens.append(curr_patient_windows.sum())        \n",
    "        \n",
    "    patient_tokens = np.asarray(patient_tokens)\n",
    "\n",
    "    #       Calculate cumulative number of recordings \n",
    "    patient_tokens_cumsum = np.cumsum(patient_tokens)\n",
    "    \n",
    "    return patient_tokens_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   EXTRACT VALUES AND LABELS FROM TRAIN/VALIDATION/TEST DATASET; TAKES FEATURES TO BE SELECTED AS ARGUMENTS TO FORWARD TO FEATURE SELECTION ALGORITHM\n",
    "#\n",
    "def data_to_values_and_labels(dfs, name):\n",
    "    dfs_values, dfs_labels = [], []\n",
    "        \n",
    "    \n",
    "    # Read in diagnosis and features/feature images from database\n",
    "    for df in dfs:\n",
    "        values = [] # (NR_OF_SAMPLE)\n",
    "        labels = [] # (NR_OF_SAMPLES,)\n",
    "            \n",
    "        rows = df.iterrows()\n",
    "        for row_index, row in rows:\n",
    "            # Return list of filenames to read in with generator\n",
    "            #       Extract file path and name from database\n",
    "            file_name = row[\"file_name\"]\n",
    "            file_path = row[\"file_path\"]\n",
    "\n",
    "            #       Add file path and name to values array\n",
    "            values.append(f\"{'data'}{file_path}{file_name}\")\n",
    "\n",
    "            #       Add patient diagnosis as label to labels array\n",
    "            if multiple_labels_dict[name]:#TODO: make sure this works\n",
    "                label_list = []\n",
    "                diagnoses = row[\"diagnosis\"]\n",
    "                diagnoses = diagnoses.split(\" \")\n",
    "                for diagnosis in diagnoses:\n",
    "                    if diagnosis == class1_dict[name]:\n",
    "                        label_list.append(1)\n",
    "                    elif diagnosis == class2_dict[name]:\n",
    "                        label_list.append(0)\n",
    "                        \n",
    "                labels.append(label_list)\n",
    "            \n",
    "            else:\n",
    "                labels.append(1 if row[\"diagnosis\"] == class1_dict[name] else 0)\n",
    "              \n",
    "        # Add extracted values and labels to arrays for returning \n",
    "        dfs_values.append(values)             \n",
    "        dfs_labels.append(labels)\n",
    "        \n",
    "        \n",
    "    return dfs_values, dfs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    # Filter out empty arrays\n",
    "    df = df_dict[dataset]\n",
    "    if not multiple_labels_dict[dataset]:\n",
    "        df = df[df[\"recording_duration\"] > 10]        \n",
    "    \n",
    "    df_train_all, df_test, df_train, df_val = train_val_test_split2(df, dataset)       \n",
    "\n",
    "    \n",
    "    #SHUFFLE\n",
    "    df_train_all = df_train_all.sample(frac=1).reset_index(drop=True)\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "    df_val = df_val.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Extract data values and labels from individual datasets\n",
    "    dfs = [df_train_all, df_test, df_train, df_val]\n",
    "    dfs_values, dfs_labels = data_to_values_and_labels(dfs, dataset) \n",
    "\n",
    "    #       Extract all values and labels from their respective datasets\n",
    "    train_all_values, train_all_labels = dfs_values[0], dfs_labels[0]\n",
    "    test_values, test_labels = dfs_values[1], dfs_labels[1]\n",
    "    train_values, train_labels = dfs_values[2], dfs_labels[2]\n",
    "    val_values, val_labels = dfs_values[3], dfs_labels[3]\n",
    "    \n",
    "    \n",
    "    train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = np.asarray(train_all_values), np.asarray(train_all_labels), np.asarray(test_values), np.asarray(test_labels), np.asarray(train_values), np.asarray(train_labels), np.asarray(val_values), np.asarray(val_labels)\n",
    "    return [train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_Data_Generator_Heterogeneous(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, file_paths, file_labels, batch_size, dataset, conv=False, class_filter = None):  \n",
    "        \n",
    "        # Store file paths and corresponding labels\n",
    "        self.file_paths = file_paths\n",
    "        self.file_names = [file_path.split(\"/\")[-1] for file_path in file_paths]\n",
    "        self.file_labels = file_labels\n",
    "        self.dataset = dataset\n",
    "        df = df_dict[dataset]\n",
    "        \n",
    "        #Store class filter and class counters\n",
    "        self.class_filter = class_filter\n",
    "        self.class0_counter = 0\n",
    "        self.class1_counter = 0\n",
    "        \n",
    "        # Store batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #initialize reshape stuff\n",
    "        self.conv = conv\n",
    "        \n",
    "        # Calculate total lentgh of the data\n",
    "        df_gen = df[df[\"file_name\"].isin(self.file_names)]\n",
    "        df_gen = df_gen.set_index(\"file_name\")\n",
    "        df_gen = df_gen.loc[self.file_names]\n",
    "    \n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            self.number_of_windows = window_list(df_gen[\"number_of_windows\"].tolist())\n",
    "            self.total_windows = total_windows(self.number_of_windows)\n",
    "        else:            \n",
    "            self.windows = np.floor((df_gen[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            self.windows_cumsum = self.windows.cumsum()\n",
    "            self.total_windows = self.windows_cumsum.iloc[-1]\n",
    "\n",
    "        # Make heterogeneously randomized list of tuples of file index and sample index in that file\n",
    "        if multiple_labels_dict[self.dataset]:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_labels = self.file_labels[file_index]\n",
    "                file_paths = []\n",
    "                for i in range(len(file_labels)):\n",
    "                    file_path = self.file_paths[file_index]+'_'+str(i)\n",
    "                    file_paths.append(file_path)\n",
    "                    for sample_index in range(self.number_of_windows[file_index][i]):\n",
    "                        file_and_sample_index.append((file_index, i, sample_index))\n",
    "                        file_and_sample_label.append(file_labels[i])\n",
    "                        \n",
    "                        if file_labels[i] == 0 and self.class0_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class0_counter += 1\n",
    "                        elif file_labels[i] == 1 and self.class1_counter < 10000:\n",
    "                            file_and_sample_index.append((file_index, i, sample_index))\n",
    "                            file_and_sample_label.append(file_labels[i])\n",
    "                            self.class1_counter += 1\n",
    "                        \n",
    "        \n",
    "        else:\n",
    "            j = 0\n",
    "            file_and_sample_index = []\n",
    "            file_and_sample_label = []\n",
    "            for file_index in range(len(self.file_paths)):\n",
    "                file_path = self.file_paths[file_index]\n",
    "                file_label = self.file_labels[file_index]\n",
    "                for sample_index in range(self.windows[file_index]):                            \n",
    "                    if file_label == 0 and self.class0_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class0_counter += 1\n",
    "                    elif file_label == 1 and self.class1_counter < 10000:\n",
    "                        file_and_sample_index.append((file_index, sample_index))\n",
    "                        file_and_sample_label.append(file_label)\n",
    "                        self.class1_counter += 1\n",
    "                       \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        #Shuffle data\n",
    "        split = list(zip(file_and_sample_index, file_and_sample_label))\n",
    "\n",
    "        random.shuffle(split)\n",
    "\n",
    "        file_and_sample_index, file_and_sample_label = list(zip(*split))\n",
    "\n",
    "        self.split_files_and_samples = []\n",
    "        self.split_labels = []\n",
    "        length = int(np.floor(len(file_and_sample_label)/batch_size))\n",
    "        \n",
    "        for i in range(length):\n",
    "            start_index = batch_size*i\n",
    "            end_index = batch_size*(i+1)\n",
    "            self.split_files_and_samples.append(np.asarray(file_and_sample_index[start_index:end_index]))\n",
    "            self.split_labels.append(np.asarray(file_and_sample_label[start_index:end_index]))\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return (np.ceil((self.class0_counter + self.class1_counter) / float(self.batch_size))-1).astype(np.int)           \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Define function to append values and labels to the return values and labels\n",
    "        def append_values_and_labels(batch_values, file_data_h5, file_path, sample_index, index=None):\n",
    "            file_name = file_path.split(\"/\")[-1][:-4]\n",
    "            if plus_edf_dict[self.dataset]:\n",
    "                file_name = file_name + '.edf'\n",
    "            \n",
    "            #Get file values\n",
    "            if index is None:\n",
    "                file_values = file_data_h5[file_name][sample_index]   \n",
    "            else:\n",
    "                file_values = file_data_h5[file_name+\"_\"+str(index)][sample_index]   \n",
    "            \n",
    "            if self.conv:\n",
    "                file_values = np.swapaxes(file_values, 0, 1)\n",
    "            else:\n",
    "                file_values = file_values.reshape((1, file_values.shape[0], file_values.shape[1]))\n",
    "            batch_values.append(file_values)\n",
    "\n",
    "            return batch_values\n",
    "        \n",
    "        # Read in values for all file paths and duplicate file labels according the the amount of values\n",
    "        #       Select subset of data\n",
    "        batch_files_and_samples = self.split_files_and_samples[idx]\n",
    "        batch_labels = self.split_labels[idx]\n",
    "        \n",
    "        #       Open raw data file\n",
    "        h5f = h5py.File(h5f_dict[self.dataset], 'r')\n",
    "\n",
    "        #       Get values for filepaths in batch\n",
    "        batch_values = []\n",
    "        \n",
    "        if  multiple_labels_dict[self.dataset]:\n",
    "            for file_path_index, index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index, index = index)\n",
    "        else:\n",
    "            for file_path_index, sample_index in batch_files_and_samples:\n",
    "                file_path = self.file_paths[file_path_index]\n",
    "                batch_values = append_values_and_labels(batch_values, file_data_h5=h5f, file_path=file_path, sample_index=sample_index)\n",
    "            \n",
    "        h5f.close()\n",
    "        self.p = np.random.permutation(len(batch_values))\n",
    "        batch_values, batch_labels = np.asarray(batch_values)[self.p], np.asarray(batch_labels)[self.p]\n",
    "        return batch_values, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split2(df, dataset):\n",
    "     # Get patient ids\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "    # Get (cumulative) number of tokens per unique epilepsy patient\n",
    "    patient_tokens_cumsum = get_tokens_cumsum(df, dataset)\n",
    "    total_tokens = np.amax(patient_tokens_cumsum)\n",
    "\n",
    "    \n",
    "    # Split patient_ids based on number of recordings; 80% of all tokens should be training data\n",
    "    # (of which 64% training data and 16% validation data) and 20% should be test data.\n",
    "    #       Find patient_index corresponding to a cumulative 80% of the data to split training from test data\n",
    "    test_split = next(index for index, curr_patient_tokens_cumsum in enumerate(patient_tokens_cumsum) if curr_patient_tokens_cumsum > 0.8 * total_tokens) + 1\n",
    "    \n",
    "    #       Use split patient_id to split off 20% of the data for test data\n",
    "    train_all_patient_ids = patient_ids[:test_split]\n",
    "    train_all_patient_tokens = patient_tokens_cumsum[:test_split]\n",
    "    \n",
    "    test_patient_ids = patient_ids[test_split:]\n",
    "    test_patient_tokens = patient_tokens_cumsum[test_split:]\n",
    "\n",
    "    #       Find patient_id corresponding to a cumulative 80% of the training data to split training data from validation data\n",
    "    total_train_tokens = np.amax(train_all_patient_tokens)\n",
    "    val_split = next(index for index, curr_patient_tokens_cumsum in enumerate(train_all_patient_tokens) if curr_patient_tokens_cumsum > 0.8 * total_train_tokens) + 1\n",
    "\n",
    "    #       Use split patient_id to split off 20% of the data for validation data\n",
    "    train_patient_ids = train_all_patient_ids[:val_split]\n",
    "    train_patient_tokens = train_all_patient_tokens[:val_split]\n",
    "\n",
    "    val_patient_ids = train_all_patient_ids[val_split:]\n",
    "    val_patient_tokens = train_all_patient_tokens[val_split:]\n",
    "\n",
    "    \n",
    "    # Split dataframe in train, validation and test sets based on the split made above\n",
    "    df_train_all = df[df[\"patient_id\"].isin(train_all_patient_ids)]\n",
    "    df_test = df[df[\"patient_id\"].isin(test_patient_ids)]\n",
    "    \n",
    "    df_train = df[df[\"patient_id\"].isin(train_patient_ids)]\n",
    "    df_val = df[df[\"patient_id\"].isin(val_patient_ids)]\n",
    "    \n",
    "    return df_train_all, df_test, df_train, df_val   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_class(df, dataset):\n",
    "    number_class0 = 0\n",
    "    number_class1 = 0\n",
    "    \n",
    "    class0 = class1_dict[dataset]\n",
    "    class1 = class2_dict[dataset]\n",
    "    for index, row in df.iterrows():\n",
    "        if not multiple_labels_dict[dataset]:\n",
    "            if class1_dict[dataset] == row[\"diagnosis\"]:\n",
    "                number_class0 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "            else:\n",
    "                number_class1 += np.floor((row[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "        else:\n",
    "            diagnoses = row[\"diagnosis\"]\n",
    "            diagnoses = diagnoses.split(\" \")\n",
    "            diagnoses.remove('')\n",
    "\n",
    "            number_of_windows = row[\"number_of_windows\"]\n",
    "            number_of_windows = number_of_windows.split(\" \")\n",
    "            number_of_windows.remove('')\n",
    "            for i in range(len(diagnoses)):\n",
    "                diagnosis = diagnoses[i]\n",
    "                windows = number_of_windows[i]\n",
    "                if diagnosis == class0:\n",
    "                    number_class0 += int(windows)\n",
    "                elif diagnosis == class1:\n",
    "                    number_class1 += int(windows)\n",
    "    return int(number_class0), int(number_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   SPLIT DATA IN ALMOST EQUAL PARTS\n",
    "#\n",
    "def stratify_ensemble_split(df, dataset, split_size):\n",
    "    #shuffle randomly\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Get patient ids\n",
    "    patient_ids = np.unique(df[\"patient_id\"])\n",
    "\n",
    "    # Get (cumulative) number of tokens per unique patient\n",
    "    patient_tokens_cumsum = get_tokens_cumsum(df, dataset)\n",
    "    \n",
    "    # take approx split size subset of df\n",
    "    split_index = next(index for index, curr_patient_tokens_cumsum in enumerate(patient_tokens_cumsum) if curr_patient_tokens_cumsum > split_size) + 1\n",
    "    stratified_ids = patient_ids[:split_index]\n",
    "    df = df[df[\"patient_id\"].isin(stratified_ids)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   HELP VISUALIZE LEARNING PROGRESSION\n",
    "#\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize = (12,16))\n",
    "    plt.subplot(4,2,1)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    try:\n",
    "        plt.plot(history.epoch, np.array(history.history['acc']),'g-',\n",
    "               label='Train accuracy')\n",
    "        plt.plot(history.epoch, np.array(history.history['val_acc']),'r-',\n",
    "               label = 'Validation accuracy')\n",
    "    except:\n",
    "        plt.plot(history.epoch, np.array(history.history['accuracy_m']),'g-',\n",
    "               label='Train accuracy')\n",
    "        plt.plot(history.epoch, np.array(history.history['val_accuracy_m']),'r-',\n",
    "               label = 'Validation accuracy')\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4,2,2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss minimised by model')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
    "           label='Train loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
    "           label = 'Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,3)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['recall_m']),'g-',\n",
    "           label='Train recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_recall_m']),'r-',\n",
    "           label = 'Validation recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,4)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_recall_m']),'g-',\n",
    "           label='Train neg. recall')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_recall_m']),'r-',\n",
    "           label = 'Validation neg. recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['precision_m']),'g-',\n",
    "           label='Train precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_precision_m']),'r-',\n",
    "           label = 'Validation precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,6)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_precision_m']),'g-',\n",
    "           label='Train neg. precision')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_precision_m']),'r-',\n",
    "           label = 'Validation neg. precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['f1_m']),'g-',\n",
    "           label='Train F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_f1_m']),'r-',\n",
    "           label = 'Validation F1')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(4,2,8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['neg_f1_m']),'g-',\n",
    "           label='Train neg. F1')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_neg_f1_m']),'r-',\n",
    "           label = 'Validation neg. F1')\n",
    "    plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   CALCULATE RECALL, PRECISION, AND F1-SCORE PERFORMANCE MEASURES\n",
    "#\n",
    "#w = 3.2552225249772935\n",
    "def w_binary_crossentropy(y_true, y_pred):\n",
    "    weights = y_true * class1_weight + (1. - y_true) * class0_weight\n",
    "    bce = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = keras.backend.mean(bce * weights)\n",
    "    return weighted_bce\n",
    "\n",
    "\n",
    "def tp_m(y_true, y_pred):\n",
    "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    return tp\n",
    "\n",
    "def fp_m(y_true, y_pred):\n",
    "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * y_pred, 0, 1)))\n",
    "    return fp\n",
    "\n",
    "def fn_m(y_true, y_pred):\n",
    "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * (1-y_pred), 0, 1)))\n",
    "    return fn\n",
    "\n",
    "def tn_m(y_true, y_pred):\n",
    "    tn = keras.backend.sum(keras.backend.round(keras.backend.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    return tn\n",
    "\n",
    "def accuracy_m(y_true, y_pred):\n",
    "    accuracy = (tp_m(y_true, y_pred) + tn_m(y_true, y_pred)) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return accuracy\n",
    "\n",
    "def pos_true_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fn_m(y_true, y_pred) \n",
    "\n",
    "def pos_pred_m(y_true, y_pred):\n",
    "    return tp_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_true_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fp_m(y_true, y_pred)\n",
    "\n",
    "def neg_pred_m(y_true, y_pred):\n",
    "    return tn_m(y_true, y_pred) + fn_m(y_true, y_pred)\n",
    "\n",
    "# Sensitivity\n",
    "def recall_m(y_true, y_pred):\n",
    "    recall = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Specificity\n",
    "def neg_recall_m(y_true, y_pred):\n",
    "    neg_recall = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    precision = tp_m(y_true, y_pred) / (tp_m(y_true, y_pred) + fp_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Negative predictive value\n",
    "def neg_precision_m(y_true, y_pred):\n",
    "    neg_precision = tn_m(y_true, y_pred) / (tn_m(y_true, y_pred) + fn_m(y_true, y_pred) + keras.backend.epsilon())\n",
    "    return neg_precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+keras.backend.epsilon()))\n",
    "\n",
    "def neg_f1_m(y_true, y_pred):\n",
    "    neg_precision = neg_precision_m(y_true, y_pred)\n",
    "    neg_recall = neg_recall_m(y_true, y_pred)\n",
    "    return 2 * ((neg_precision * neg_recall) / (neg_precision + neg_recall + keras.backend.epsilon()))\n",
    "\n",
    "def balanced_acc_m(y_true, y_pred):\n",
    "    return (recall_m(y_true, y_pred) + neg_recall_m(y_true, y_pred))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    y_actual = np.asarray(y_actual)\n",
    "    y_hat = np.asarray(y_hat)\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(y_hat)): \n",
    "        if len(y_actual.shape) == 2 and y_actual.shape[1] == 2:\n",
    "            y_actual_ = np.argmax(y_actual[i])\n",
    "        else:\n",
    "            y_actual_ = y_actual[i]\n",
    "        \n",
    "        if len(y_hat.shape) == 2 and y_hat.shape[1] == 2:\n",
    "            y_hat_ = np.argmax(y_hat[i])\n",
    "        else:\n",
    "            y_hat_ = y_hat[i]\n",
    "            \n",
    "        if y_actual_==y_hat_==1:\n",
    "            TP += 1\n",
    "        if y_hat_==1 and y_actual_!=y_hat_:\n",
    "            FP += 1\n",
    "        if y_actual_==y_hat_==0:\n",
    "            TN += 1\n",
    "        if y_hat_==0 and y_actual_!=y_hat_:\n",
    "            FN += 1\n",
    "    \n",
    "    print(f\"tp({TP}), fp({FP}), tn({TN}), fn({FN})\")\n",
    "    \n",
    "    accuracy = (TP + TN) / (TN + TP + FP + FN)\n",
    "    recall = TP / (FN + TP)\n",
    "    neg_recall = TN / (FP + TN)\n",
    "    \n",
    "    try:\n",
    "        precision = TP / (FP + TP)\n",
    "    except:\n",
    "        print(\"No positive predictions...\")\n",
    "        precision = 0\n",
    "        \n",
    "    try:\n",
    "        neg_precision = TN / (FN + TN)\n",
    "    except:\n",
    "        print(\"No negative predictions...\")\n",
    "        neg_precision = 0\n",
    "            \n",
    "    try:\n",
    "        f1 = 2 * (recall * precision) / (recall + precision)\n",
    "    except:\n",
    "        f1 = 0\n",
    "        \n",
    "    try:\n",
    "        neg_f1 = 2 * (neg_recall * neg_precision) / (neg_recall + neg_precision)\n",
    "    except:\n",
    "        neg_f1 = 0\n",
    "\n",
    "    aupr = average_precision_score(np.asarray(y_actual), np.asarray(y_hat))\n",
    "    neg_aupr = average_precision_score(np.asarray([1 - y_a for y_a in y_actual]), np.asarray([1 - y_h for y_h in y_hat]))\n",
    "\n",
    "    print(f\"acc={accuracy}, recall={recall}, neg_recall={neg_recall}, precision={precision}, neg_precision={neg_precision}, f1={f1}, neg_f1={neg_f1}, aupr={aupr}, neg_aupr={neg_aupr}\")\n",
    "    \n",
    "    return accuracy, recall, neg_recall, precision, neg_precision, f1, neg_f1, aupr, neg_aupr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(dataset, file_paths, file_labels):\n",
    "    #number of windows of label, number of windows of window \n",
    "    class0 = 0\n",
    "    class1 = 0\n",
    "    \n",
    "    # Store file paths and corresponding labels\n",
    "    file_names = [file_path.split(\"/\")[-1] for file_path in file_paths]\n",
    "    df = df_dict[dataset]\n",
    "        \n",
    "    # Calculate total lentgh of the data\n",
    "    df_gen = df[df[\"file_name\"].isin(file_names)]\n",
    "    df_gen = df_gen.set_index(\"file_name\")\n",
    "    df_gen = df_gen.loc[file_names]\n",
    "    \n",
    "    #calc number of windows\n",
    "    if multiple_labels_dict[dataset]:\n",
    "        number_of_windows = window_list(df_gen[\"number_of_windows\"].tolist())\n",
    "    else:            \n",
    "        windows = np.floor((df_gen[\"recording_duration\"] - 1) / 10).astype(int)\n",
    "        \n",
    "   # Make heterogeneously randomized list of tuples of file index and sample index in that file\n",
    "    if multiple_labels_dict[dataset]:\n",
    "        for file_index in range(len(file_paths)):\n",
    "            this_file_labels = file_labels[file_index]\n",
    "            for i in range(len(this_file_labels)):\n",
    "                if this_file_labels[i] == 0:\n",
    "                    class0 += number_of_windows[file_index][i]\n",
    "                else:\n",
    "                    class1 += number_of_windows[file_index][i]\n",
    "                        \n",
    "        \n",
    "    else:\n",
    "        for file_index in range(len(file_paths)):\n",
    "            file_label = file_labels[file_index]\n",
    "            if file_label == 0:\n",
    "                class0 += windows[file_index]\n",
    "            else:\n",
    "                class1 += windows[file_index]    \n",
    "    \n",
    "    total = class0 + class1\n",
    "    class0_weight = class1/total\n",
    "    class1_weight = class0/total\n",
    "    return {0 : class0_weight, 1 : class1_weight}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(h5f_dict[\"TUSZ\"], 'r')\n",
    "#print(h5f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0.5, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 1, 19, 2560)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 19, 4)          1310724   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 19, 4)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1, 19, 4)          4         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 2, 1, 4)           40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 1, 4)           8         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d (SeparableC (None, 2, 1, 8)           24        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 1, 8)           8         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 1, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,310,811\n",
      "Trainable params: 1,310,801\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = get_eegnet()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUEP\"\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_values, train_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Model\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = class_weights(dataset, train_all_values, train_all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6012498915024738, 1: 0.39875010849752623}\n"
     ]
    }
   ],
   "source": [
    "print(class_weights(dataset, val_values, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9087066650390625e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-3bf51cacc54b>:20: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      " 2/71 [..............................] - ETA: 4s - loss: 0.6937 - balanced_acc_m: 0.4897 - pos_pred_m: 246.5000 - neg_pred_m: 9.5000 - tp_m: 121.5000 - fp_m: 125.0000 - tn_m: 3.0000 - fn_m: 6.5000 - recall_m: 0.9536 - precision_m: 0.4945  WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0379s vs `on_train_batch_end` time: 0.1003s). Check your callbacks.\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6869 - balanced_acc_m: 0.4997 - pos_pred_m: 255.7324 - neg_pred_m: 0.2676 - tp_m: 139.3944 - fp_m: 116.3380 - tn_m: 0.0845 - fn_m: 0.1831 - recall_m: 0.9987 - precision_m: 0.5451\n",
      "Epoch 00001: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/speed_test.hdf5\n",
      "71/71 [==============================] - 97s 1s/step - loss: 0.6869 - balanced_acc_m: 0.4997 - pos_pred_m: 255.7324 - neg_pred_m: 0.2676 - tp_m: 139.3944 - fp_m: 116.3380 - tn_m: 0.0845 - fn_m: 0.1831 - recall_m: 0.9987 - precision_m: 0.5451 - val_loss: 0.6923 - val_balanced_acc_m: 0.5000 - val_pos_pred_m: 256.0000 - val_neg_pred_m: 0.0000e+00 - val_tp_m: 133.4730 - val_fp_m: 122.5270 - val_tn_m: 0.0000e+00 - val_fn_m: 0.0000e+00 - val_recall_m: 1.0000 - val_precision_m: 0.5214\n",
      "106.35384917259216\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs = 1\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/speed_test.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/logbooks/logbook_speed_test.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          validation_data=val_generator,\n",
    "                                          validation_steps=len(val_generator),\n",
    "                                          #class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusz_baseline_a_3.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 34s 748ms/step - loss: 3.3526 - balanced_acc_m: 0.5820 - pos_pred_m: 67.4222 - neg_pred_m: 188.5778 - tp_m: 9.6222 - fp_m: 57.8000 - tn_m: 174.8222 - fn_m: 13.7556 - recall_m: 0.4123 - precision_m: 0.1429\n",
      "WARNING:tensorflow:From <ipython-input-46-fc531138a767>:2: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "balanced accuracy 0.5819920301437378\n",
      "aupr 0.09463586489666856\n",
      "recall 0.4123237133026123 precision 0.1429234743118286 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"balanced accuracy\", test_dict['balanced_acc_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0.5, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUAB\"\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_values, train_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Model\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = class_weights(dataset, train_values, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tuab_val_400epochs.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/logbooks/logbook_tuab_val_400epochs.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          validation_data=val_generator,\n",
    "                                          validation_steps=len(val_generator),\n",
    "                                          #class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"balanced accuracy\", test_dict['balanced_acc_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0.5, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUSZ\"\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_values, train_labels, batch_size, dataset, class_filter = None)\n",
    "val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Model\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = class_weights(dataset, train_values, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusz_val_b_0.5.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/logbooks/logbook_tusz_val_b_0.5.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          validation_data=val_generator,\n",
    "                                          validation_steps=len(val_generator),\n",
    "                                          #class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegnet():\n",
    "    \n",
    "    F1 = 4\n",
    "    C = 19\n",
    "    D = 2\n",
    "    F2 = D * F1\n",
    "\n",
    "    do = [0.25, 0.25, 0]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(do[0],input_shape=(1, C, 2560)))\n",
    "    model.add(Conv2D(filters=F1, kernel_size=(1, 128), input_shape=(1, C, 2560), padding=\"same\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(C, 1), padding=\"valid\", depth_multiplier=D, data_format='channels_first'))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), data_format='channels_first'))\n",
    "    model.add(Dropout(do[1]))\n",
    "\n",
    "    model.add(SeparableConv2D(filters=F2, kernel_size=(1, 16), padding=\"same\", use_bias=False))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation(\"elu\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(1, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(do[2]))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUSL\"\n",
    "batch_size = 256\n",
    "\n",
    "train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels = get_data(dataset)\n",
    "\n",
    "#Generator\n",
    "train_generator = EEG_Data_Generator_Heterogeneous(train_all_values, train_all_labels, batch_size, dataset, class_filter = None)\n",
    "#val_generator = EEG_Data_Generator_Heterogeneous(val_values, val_labels, batch_size, dataset, class_filter = None)\n",
    "\n",
    "#Model\n",
    "my_model = get_eegnet()\n",
    "\n",
    "#Weights\n",
    "class_weights_dict = class_weights(dataset, train_all_values, train_all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-15633aa6a1af>:18: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3408 - balanced_acc_m: 0.5027 - pos_pred_m: 52.9600 - neg_pred_m: 203.0400 - tp_m: 23.2933 - fp_m: 29.6667 - tn_m: 115.4800 - fn_m: 87.5600 - recall_m: 0.2102 - precision_m: 0.1922\n",
      "Epoch 00001: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 46s 617ms/step - loss: 0.3408 - balanced_acc_m: 0.5027 - pos_pred_m: 52.9600 - neg_pred_m: 203.0400 - tp_m: 23.2933 - fp_m: 29.6667 - tn_m: 115.4800 - fn_m: 87.5600 - recall_m: 0.2102 - precision_m: 0.1922\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3402 - balanced_acc_m: 0.5032 - pos_pred_m: 115.7600 - neg_pred_m: 140.2400 - tp_m: 50.2000 - fp_m: 65.5600 - tn_m: 79.5867 - fn_m: 60.6533 - recall_m: 0.4559 - precision_m: 0.3797\n",
      "Epoch 00002: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.3402 - balanced_acc_m: 0.5032 - pos_pred_m: 115.7600 - neg_pred_m: 140.2400 - tp_m: 50.2000 - fp_m: 65.5600 - tn_m: 79.5867 - fn_m: 60.6533 - recall_m: 0.4559 - precision_m: 0.3797\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3398 - balanced_acc_m: 0.5128 - pos_pred_m: 150.9467 - neg_pred_m: 105.0533 - tp_m: 66.9200 - fp_m: 84.0267 - tn_m: 61.1200 - fn_m: 43.9333 - recall_m: 0.6042 - precision_m: 0.4440\n",
      "Epoch 00003: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 0.3398 - balanced_acc_m: 0.5128 - pos_pred_m: 150.9467 - neg_pred_m: 105.0533 - tp_m: 66.9200 - fp_m: 84.0267 - tn_m: 61.1200 - fn_m: 43.9333 - recall_m: 0.6042 - precision_m: 0.4440\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3393 - balanced_acc_m: 0.5273 - pos_pred_m: 162.2800 - neg_pred_m: 93.7200 - tp_m: 73.6533 - fp_m: 88.6267 - tn_m: 56.5200 - fn_m: 37.2000 - recall_m: 0.6647 - precision_m: 0.4543\n",
      "Epoch 00004: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.3393 - balanced_acc_m: 0.5273 - pos_pred_m: 162.2800 - neg_pred_m: 93.7200 - tp_m: 73.6533 - fp_m: 88.6267 - tn_m: 56.5200 - fn_m: 37.2000 - recall_m: 0.6647 - precision_m: 0.4543\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3381 - balanced_acc_m: 0.5412 - pos_pred_m: 148.1867 - neg_pred_m: 107.8000 - tp_m: 69.2667 - fp_m: 78.9200 - tn_m: 66.2133 - fn_m: 41.5867 - recall_m: 0.6259 - precision_m: 0.4684\n",
      "Epoch 00005: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 0.3381 - balanced_acc_m: 0.5412 - pos_pred_m: 148.1867 - neg_pred_m: 107.8000 - tp_m: 69.2667 - fp_m: 78.9200 - tn_m: 66.2133 - fn_m: 41.5867 - recall_m: 0.6259 - precision_m: 0.4684\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3353 - balanced_acc_m: 0.5611 - pos_pred_m: 148.5067 - neg_pred_m: 107.4933 - tp_m: 71.9333 - fp_m: 76.5733 - tn_m: 68.5733 - fn_m: 38.9200 - recall_m: 0.6498 - precision_m: 0.4849\n",
      "Epoch 00006: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 0.3353 - balanced_acc_m: 0.5611 - pos_pred_m: 148.5067 - neg_pred_m: 107.4933 - tp_m: 71.9333 - fp_m: 76.5733 - tn_m: 68.5733 - fn_m: 38.9200 - recall_m: 0.6498 - precision_m: 0.4849\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3309 - balanced_acc_m: 0.5836 - pos_pred_m: 142.7067 - neg_pred_m: 113.2933 - tp_m: 72.2400 - fp_m: 70.4667 - tn_m: 74.6800 - fn_m: 38.6133 - recall_m: 0.6525 - precision_m: 0.5074\n",
      "Epoch 00007: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 0.3309 - balanced_acc_m: 0.5836 - pos_pred_m: 142.7067 - neg_pred_m: 113.2933 - tp_m: 72.2400 - fp_m: 70.4667 - tn_m: 74.6800 - fn_m: 38.6133 - recall_m: 0.6525 - precision_m: 0.5074\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3268 - balanced_acc_m: 0.5977 - pos_pred_m: 142.7467 - neg_pred_m: 113.2533 - tp_m: 74.0667 - fp_m: 68.6800 - tn_m: 76.4667 - fn_m: 36.7867 - recall_m: 0.6685 - precision_m: 0.5193\n",
      "Epoch 00008: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 0.3268 - balanced_acc_m: 0.5977 - pos_pred_m: 142.7467 - neg_pred_m: 113.2533 - tp_m: 74.0667 - fp_m: 68.6800 - tn_m: 76.4667 - fn_m: 36.7867 - recall_m: 0.6685 - precision_m: 0.5193\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3208 - balanced_acc_m: 0.6134 - pos_pred_m: 137.3200 - neg_pred_m: 118.6800 - tp_m: 73.6667 - fp_m: 63.6533 - tn_m: 81.4933 - fn_m: 37.1867 - recall_m: 0.6650 - precision_m: 0.5377\n",
      "Epoch 00009: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 0.3208 - balanced_acc_m: 0.6134 - pos_pred_m: 137.3200 - neg_pred_m: 118.6800 - tp_m: 73.6667 - fp_m: 63.6533 - tn_m: 81.4933 - fn_m: 37.1867 - recall_m: 0.6650 - precision_m: 0.5377\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3164 - balanced_acc_m: 0.6270 - pos_pred_m: 139.9600 - neg_pred_m: 116.0400 - tp_m: 76.5733 - fp_m: 63.3867 - tn_m: 81.7600 - fn_m: 34.2800 - recall_m: 0.6909 - precision_m: 0.5481\n",
      "Epoch 00010: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 0.3164 - balanced_acc_m: 0.6270 - pos_pred_m: 139.9600 - neg_pred_m: 116.0400 - tp_m: 76.5733 - fp_m: 63.3867 - tn_m: 81.7600 - fn_m: 34.2800 - recall_m: 0.6909 - precision_m: 0.5481\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3110 - balanced_acc_m: 0.6410 - pos_pred_m: 137.3600 - neg_pred_m: 118.6400 - tp_m: 77.1600 - fp_m: 60.2000 - tn_m: 84.9467 - fn_m: 33.6933 - recall_m: 0.6965 - precision_m: 0.5624\n",
      "Epoch 00011: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 0.3110 - balanced_acc_m: 0.6410 - pos_pred_m: 137.3600 - neg_pred_m: 118.6400 - tp_m: 77.1600 - fp_m: 60.2000 - tn_m: 84.9467 - fn_m: 33.6933 - recall_m: 0.6965 - precision_m: 0.5624\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3068 - balanced_acc_m: 0.6440 - pos_pred_m: 138.2800 - neg_pred_m: 117.7200 - tp_m: 77.9200 - fp_m: 60.3600 - tn_m: 84.7867 - fn_m: 32.9333 - recall_m: 0.7034 - precision_m: 0.5649\n",
      "Epoch 00012: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 0.3068 - balanced_acc_m: 0.6440 - pos_pred_m: 138.2800 - neg_pred_m: 117.7200 - tp_m: 77.9200 - fp_m: 60.3600 - tn_m: 84.7867 - fn_m: 32.9333 - recall_m: 0.7034 - precision_m: 0.5649\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3017 - balanced_acc_m: 0.6604 - pos_pred_m: 130.0800 - neg_pred_m: 125.9200 - tp_m: 76.4133 - fp_m: 53.6667 - tn_m: 91.4800 - fn_m: 34.4400 - recall_m: 0.6905 - precision_m: 0.5900\n",
      "Epoch 00013: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 0.3017 - balanced_acc_m: 0.6604 - pos_pred_m: 130.0800 - neg_pred_m: 125.9200 - tp_m: 76.4133 - fp_m: 53.6667 - tn_m: 91.4800 - fn_m: 34.4400 - recall_m: 0.6905 - precision_m: 0.5900\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2969 - balanced_acc_m: 0.6699 - pos_pred_m: 131.0267 - neg_pred_m: 124.9733 - tp_m: 78.0000 - fp_m: 53.0267 - tn_m: 92.1200 - fn_m: 32.8533 - recall_m: 0.7046 - precision_m: 0.5989\n",
      "Epoch 00014: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 0.2969 - balanced_acc_m: 0.6699 - pos_pred_m: 131.0267 - neg_pred_m: 124.9733 - tp_m: 78.0000 - fp_m: 53.0267 - tn_m: 92.1200 - fn_m: 32.8533 - recall_m: 0.7046 - precision_m: 0.5989\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2950 - balanced_acc_m: 0.6699 - pos_pred_m: 133.2133 - neg_pred_m: 122.7867 - tp_m: 78.9733 - fp_m: 54.2400 - tn_m: 90.9067 - fn_m: 31.8800 - recall_m: 0.7132 - precision_m: 0.5956\n",
      "Epoch 00015: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 0.2950 - balanced_acc_m: 0.6699 - pos_pred_m: 133.2133 - neg_pred_m: 122.7867 - tp_m: 78.9733 - fp_m: 54.2400 - tn_m: 90.9067 - fn_m: 31.8800 - recall_m: 0.7132 - precision_m: 0.5956\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2921 - balanced_acc_m: 0.6779 - pos_pred_m: 132.3467 - neg_pred_m: 123.6533 - tp_m: 79.5867 - fp_m: 52.7600 - tn_m: 92.3867 - fn_m: 31.2667 - recall_m: 0.7187 - precision_m: 0.6050\n",
      "Epoch 00016: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 0.2921 - balanced_acc_m: 0.6779 - pos_pred_m: 132.3467 - neg_pred_m: 123.6533 - tp_m: 79.5867 - fp_m: 52.7600 - tn_m: 92.3867 - fn_m: 31.2667 - recall_m: 0.7187 - precision_m: 0.6050\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2866 - balanced_acc_m: 0.6878 - pos_pred_m: 129.5467 - neg_pred_m: 126.4533 - tp_m: 79.6933 - fp_m: 49.8533 - tn_m: 95.2933 - fn_m: 31.1600 - recall_m: 0.7189 - precision_m: 0.6180\n",
      "Epoch 00017: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 0.2866 - balanced_acc_m: 0.6878 - pos_pred_m: 129.5467 - neg_pred_m: 126.4533 - tp_m: 79.6933 - fp_m: 49.8533 - tn_m: 95.2933 - fn_m: 31.1600 - recall_m: 0.7189 - precision_m: 0.6180\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2849 - balanced_acc_m: 0.6881 - pos_pred_m: 131.3600 - neg_pred_m: 124.6400 - tp_m: 80.5067 - fp_m: 50.8533 - tn_m: 94.2933 - fn_m: 30.3467 - recall_m: 0.7265 - precision_m: 0.6159\n",
      "Epoch 00018: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 0.2849 - balanced_acc_m: 0.6881 - pos_pred_m: 131.3600 - neg_pred_m: 124.6400 - tp_m: 80.5067 - fp_m: 50.8533 - tn_m: 94.2933 - fn_m: 30.3467 - recall_m: 0.7265 - precision_m: 0.6159\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2818 - balanced_acc_m: 0.6972 - pos_pred_m: 131.1600 - neg_pred_m: 124.8400 - tp_m: 81.5067 - fp_m: 49.6533 - tn_m: 95.4933 - fn_m: 29.3467 - recall_m: 0.7359 - precision_m: 0.6243\n",
      "Epoch 00019: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 0.2818 - balanced_acc_m: 0.6972 - pos_pred_m: 131.1600 - neg_pred_m: 124.8400 - tp_m: 81.5067 - fp_m: 49.6533 - tn_m: 95.4933 - fn_m: 29.3467 - recall_m: 0.7359 - precision_m: 0.6243\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2790 - balanced_acc_m: 0.7029 - pos_pred_m: 126.6533 - neg_pred_m: 129.3467 - tp_m: 80.2533 - fp_m: 46.4000 - tn_m: 98.7467 - fn_m: 30.6000 - recall_m: 0.7248 - precision_m: 0.6384\n",
      "Epoch 00020: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 0.2790 - balanced_acc_m: 0.7029 - pos_pred_m: 126.6533 - neg_pred_m: 129.3467 - tp_m: 80.2533 - fp_m: 46.4000 - tn_m: 98.7467 - fn_m: 30.6000 - recall_m: 0.7248 - precision_m: 0.6384\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2771 - balanced_acc_m: 0.7084 - pos_pred_m: 128.3867 - neg_pred_m: 127.6133 - tp_m: 81.7867 - fp_m: 46.6000 - tn_m: 98.5467 - fn_m: 29.0667 - recall_m: 0.7378 - precision_m: 0.6408\n",
      "Epoch 00021: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 0.2771 - balanced_acc_m: 0.7084 - pos_pred_m: 128.3867 - neg_pred_m: 127.6133 - tp_m: 81.7867 - fp_m: 46.6000 - tn_m: 98.5467 - fn_m: 29.0667 - recall_m: 0.7378 - precision_m: 0.6408\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2741 - balanced_acc_m: 0.7100 - pos_pred_m: 128.3600 - neg_pred_m: 127.6400 - tp_m: 81.9467 - fp_m: 46.4133 - tn_m: 98.7333 - fn_m: 28.9067 - recall_m: 0.7399 - precision_m: 0.6432\n",
      "Epoch 00022: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 0.2741 - balanced_acc_m: 0.7100 - pos_pred_m: 128.3600 - neg_pred_m: 127.6400 - tp_m: 81.9467 - fp_m: 46.4133 - tn_m: 98.7333 - fn_m: 28.9067 - recall_m: 0.7399 - precision_m: 0.6432\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2738 - balanced_acc_m: 0.7094 - pos_pred_m: 131.9600 - neg_pred_m: 124.0400 - tp_m: 83.4667 - fp_m: 48.4933 - tn_m: 96.6533 - fn_m: 27.3867 - recall_m: 0.7531 - precision_m: 0.6370\n",
      "Epoch 00023: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 0.2738 - balanced_acc_m: 0.7094 - pos_pred_m: 131.9600 - neg_pred_m: 124.0400 - tp_m: 83.4667 - fp_m: 48.4933 - tn_m: 96.6533 - fn_m: 27.3867 - recall_m: 0.7531 - precision_m: 0.6370\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2716 - balanced_acc_m: 0.7167 - pos_pred_m: 129.0533 - neg_pred_m: 126.9467 - tp_m: 83.0400 - fp_m: 46.0133 - tn_m: 99.1333 - fn_m: 27.8133 - recall_m: 0.7498 - precision_m: 0.6479\n",
      "Epoch 00024: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 0.2716 - balanced_acc_m: 0.7167 - pos_pred_m: 129.0533 - neg_pred_m: 126.9467 - tp_m: 83.0400 - fp_m: 46.0133 - tn_m: 99.1333 - fn_m: 27.8133 - recall_m: 0.7498 - precision_m: 0.6479\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2705 - balanced_acc_m: 0.7156 - pos_pred_m: 127.4933 - neg_pred_m: 128.5067 - tp_m: 82.3200 - fp_m: 45.1733 - tn_m: 99.9733 - fn_m: 28.5333 - recall_m: 0.7424 - precision_m: 0.6501\n",
      "Epoch 00025: saving model to /home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 0.2705 - balanced_acc_m: 0.7156 - pos_pred_m: 127.4933 - neg_pred_m: 128.5067 - tp_m: 82.3200 - fp_m: 45.1733 - tn_m: 99.9733 - fn_m: 28.5333 - recall_m: 0.7424 - precision_m: 0.6501\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "checkpoint_path = '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/model_weights/tusl_baseline.hdf5'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_balanced_acc_m',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "log_callback = tf.keras.callbacks.CSVLogger(filename =  '/home/jupyter/time_series_transfer_learning/transfer_learning/0-baseline/logbooks/logbook_tusl_baseline.csv', separator=\",\", append=True)\n",
    "active_callbacks = [cp_callback, log_callback]\n",
    "\n",
    "\n",
    "my_model_history = my_model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=len(train_generator),\n",
    "                                          epochs=epochs,\n",
    "                                          shuffle=True,\n",
    "                                          #validation_data=val_generator,\n",
    "                                          #validation_steps=len(val_generator),\n",
    "                                          class_weight= class_weights_dict,\n",
    "                                        callbacks = active_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 9s 624ms/step - loss: 0.8514 - balanced_acc_m: 0.4677 - pos_pred_m: 148.1333 - neg_pred_m: 107.8667 - tp_m: 44.3333 - fp_m: 103.8000 - tn_m: 69.6000 - fn_m: 38.2667 - recall_m: 0.5339 - precision_m: 0.2987\n",
      "WARNING:tensorflow:From <ipython-input-31-fc531138a767>:2: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "balanced accuracy 0.467720627784729\n",
      "aupr 0.324736371637266\n",
      "recall 0.5338835120201111 precision 0.29868078231811523 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"balanced accuracy\", test_dict['balanced_acc_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   HELP VISUALIZE LEARNING PROGRESSION\n",
    "#\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize = (12,16))\n",
    "    plt.subplot(2,2,1)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    plt.plot(history.epoch, np.array(history.history['balanced_acc_m']),'g-', label='Train accuracy')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_balanced_acc_m']),'r-', label = 'Validation accuracy')\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss minimised by model')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
    "           label='Train loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
    "           label = 'Validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_weights(checkpoint_path)\n",
    "test_generator = EEG_Data_Generator_Heterogeneous(test_values, test_labels, batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 88ms/step - loss: 0.7348 - accuracy_m: 0.3411 - recall_m: 0.3282 - neg_recall_m: 0.6694 - precision_m: 0.9626 - neg_precision_m: 0.0369 - f1_m: 0.4879 - neg_f1_m: 0.0696 - pos_pred_m: 42.0000 - neg_pred_m: 86.0000\n",
      "accuracy 0.3411458432674408\n",
      "aupr 0.9678087810374134\n",
      "recall 0.32824501395225525 precision 0.9625529646873474 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dict = my_model.evaluate(test_generator, steps=len(test_generator), return_dict = True)\n",
    "test_preds = my_model.predict_generator(test_generator, steps=len(test_generator))\n",
    "test_labels_ = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "\n",
    "print(\"accuracy\", test_dict['accuracy_m'])\n",
    "print(\"aupr\", average_precision_score(test_labels_, test_preds, average=\"micro\"))\n",
    "print(\"recall\", test_dict['recall_m'], \"precision\", test_dict['precision_m'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1719666419223027\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "dataset = \"TUSZ\"\n",
    "df = df_dict[dataset]\n",
    "\n",
    "windows_class1 = 0\n",
    "windows_class2 = 0\n",
    "\n",
    "# Gather all epileptic and non-epileptic patients in separate dataframes\n",
    "if dataset == \"TUEP\":\n",
    "    df_class1 = df[df[\"diagnosis\"]==\"epilepsy\"]\n",
    "    df_class2 = df[df[\"diagnosis\"]==\"no_epilepsy\"]\n",
    "elif dataset == \"TUAB\":\n",
    "    df_class1 = df[df[\"diagnosis\"]==\"abnormal\"]\n",
    "    df_class2 = df[df[\"diagnosis\"]==\"normal\"]\n",
    "\n",
    "# Calculate windows and how many of each class there are\n",
    "if dataset == \"TUEP\" or dataset == \"TUAB\":\n",
    "    class1_cumsum = get_tokens_cumsum(df_class1, dataset)\n",
    "    class2_cumsum = get_tokens_cumsum(df_class2, dataset)\n",
    "    print(np.amax(class1_cumsum), np.amax(class2_cumsum))\n",
    "\n",
    "else:#TUSZ \n",
    "    print(window_sum_class(df))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>number_of_windows</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>electrode_setup</th>\n",
       "      <th>recording_duration</th>\n",
       "      <th>sampling_freq</th>\n",
       "      <th>len_of_samples</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s002</td>\n",
       "      <td>00000258_s002_t000</td>\n",
       "      <td>5</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>20.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s002_t000.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s002_2003_07_21/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s002</td>\n",
       "      <td>00000258_s002_t002</td>\n",
       "      <td>105</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>272.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s002_t002.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s002_2003_07_21/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t000</td>\n",
       "      <td>90</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>234.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t000.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t001</td>\n",
       "      <td>86</td>\n",
       "      <td>bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>224.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t001.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000258</td>\n",
       "      <td>s003</td>\n",
       "      <td>00000258_s003_t002</td>\n",
       "      <td>0 62 8</td>\n",
       "      <td>bckg seiz bckg</td>\n",
       "      <td>01_tcp_ar</td>\n",
       "      <td>203.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>00000258_s003_t002.edf</td>\n",
       "      <td>/dev/01_tcp_ar/002/00000258/s003_2003_07_22/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id session_id            token_id number_of_windows  \\\n",
       "0   00000258       s002  00000258_s002_t000                 5   \n",
       "1   00000258       s002  00000258_s002_t002               105   \n",
       "2   00000258       s003  00000258_s003_t000                90   \n",
       "3   00000258       s003  00000258_s003_t001                86   \n",
       "4   00000258       s003  00000258_s003_t002            0 62 8   \n",
       "\n",
       "         diagnosis electrode_setup  recording_duration  sampling_freq  \\\n",
       "0            bckg        01_tcp_ar                20.0            400   \n",
       "1            bckg        01_tcp_ar               272.0            400   \n",
       "2            bckg        01_tcp_ar               234.0            400   \n",
       "3            bckg        01_tcp_ar               224.0            400   \n",
       "4  bckg seiz bckg        01_tcp_ar               203.0            400   \n",
       "\n",
       "   len_of_samples               file_name  \\\n",
       "0               1  00000258_s002_t000.edf   \n",
       "1               1  00000258_s002_t002.edf   \n",
       "2               1  00000258_s003_t000.edf   \n",
       "3               1  00000258_s003_t001.edf   \n",
       "4               1  00000258_s003_t002.edf   \n",
       "\n",
       "                                      file_path  \n",
       "0  /dev/01_tcp_ar/002/00000258/s002_2003_07_21/  \n",
       "1  /dev/01_tcp_ar/002/00000258/s002_2003_07_21/  \n",
       "2  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  \n",
       "3  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  \n",
       "4  /dev/01_tcp_ar/002/00000258/s003_2003_07_22/  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"TUSZ\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sum_class(l):\n",
    "    #number_of_windows = l['number_of_windows']\n",
    "    windows_class1 = 0\n",
    "    windows_class2 = 0\n",
    "    \n",
    "    for index, data in l.iterrows():\n",
    "        windows = data[\"number_of_windows\"]\n",
    "        windows = windows.split(\" \")\n",
    "        \n",
    "        diagnoses = data[\"diagnosis\"]\n",
    "        diagnoses = diagnoses.split(\" \")\n",
    "    \n",
    "        for i in range(len(windows)):\n",
    "            if not windows[i] == '':\n",
    "                if diagnoses[i] == \"seiz\":\n",
    "                    windows_class1 += int(windows[i])\n",
    "                else:\n",
    "                    windows_class2 += int(windows[i])\n",
    "    return windows_class1, windows_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'number_of_windows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'number_of_windows'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b5d81f5707d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_sum_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_TUEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-c2db8aeadfa9>\u001b[0m in \u001b[0;36mwindow_sum_class\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"number_of_windows\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'number_of_windows'"
     ]
    }
   ],
   "source": [
    "print(window_sum_class(df_TUEP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "dataset = \"TUSZ\"\n",
    "i = 0\n",
    "#train_all_values, train_all_labels, test_values, test_labels, train_values, train_labels, val_values, val_labels, class1_filters = get_data(dataset)\n",
    "h5f = h5py.File(h5f_dict[dataset], 'r')\n",
    "file_name = train_values[i].split(\"/\")[-1]\n",
    "file_name = file_name.split(\"/\")[-1][:-4] #+ \".edf_0\"\n",
    "print(file_name)\n",
    "print(train_labels[i])\n",
    "print(h5f[file_name].shape)\n",
    "plt.plot(h5f[file_name][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dense():\n",
    "    model = Sequential()\n",
    "    \n",
    "    do = [0, 0]\n",
    "    \n",
    "    #Block 1\n",
    "    model.add(Flatten(input_shape=(1, 19, 2560)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(do[1]))\n",
    "    \n",
    "    #Block 2\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dropout(do[1]))\n",
    "    \n",
    "    #Block 3\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=[balanced_acc_m, pos_pred_m, neg_pred_m, tp_m, fp_m, tn_m, fn_m, recall_m, precision_m])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "file_extension": ".py",
  "interpreter": {
   "hash": "1ccaac5b7a573f1cdba5f7b708c2c32d325a2cd4ec145a76750e10d63a8e887c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
